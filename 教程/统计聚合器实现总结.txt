================================================================================
统计聚合器实现完成 - 方案一（定时聚合）
================================================================================

【实现内容】

✅ 1. 核心聚合器程序
   - backend/stats_aggregator/aggregator.py
   - 自动定时聚合（守护进程模式）
   - 支持单次执行（测试模式）
   - 可配置聚合间隔（默认30分钟）

✅ 2. 聚合逻辑
   - 从 botnet_nodes_{type} 读取原始节点数据
   - 按省市聚合 → china_botnet_{type}
   - 按国家聚合 → global_botnet_{type}
   - 自动创建统计表（如果不存在）

✅ 3. 启动脚本
   - start_aggregator.bat (Windows)
   - start_aggregator.sh (Linux/Mac)
   - start_all_services.bat (Windows一键启动所有服务)
   - start_all_services.sh (Linux/Mac一键启动所有服务)
   - stop_all_services.sh (Linux/Mac停止所有服务)

✅ 4. 测试工具
   - test_aggregator.py - 完整功能测试
   - 测试数据库连接、节点表、聚合功能、统计表

✅ 5. 文档
   - backend/stats_aggregator/README.md - 详细技术文档
   - 统计聚合器使用指南.md - 快速上手指南
   - 统计聚合器实现总结.txt - 本文件

【使用方法】

方法1: 测试聚合功能
   cd backend
   python test_aggregator.py

方法2: 单次执行（测试/手动触发）
   cd backend
   python stats_aggregator/aggregator.py once              # 聚合所有类型
   python stats_aggregator/aggregator.py once mozi         # 只聚合mozi

方法3: 守护进程模式（生产环境）
   cd backend
   
   # Windows
   start_aggregator.bat
   
   # Linux/Mac
   chmod +x start_aggregator.sh
   ./start_aggregator.sh
   
   # 或直接运行
   python stats_aggregator/aggregator.py daemon 30         # 每30分钟
   python stats_aggregator/aggregator.py daemon 5          # 每5分钟

方法4: 一键启动所有服务（推荐）
   # Windows
   start_all_services.bat
   
   # Linux/Mac
   chmod +x start_all_services.sh
   chmod +x stop_all_services.sh
   ./start_all_services.sh
   
   # 停止所有服务
   ./stop_all_services.sh

【核心特性】

✨ 代码内置定时器 - 无需手动设置crontab/任务计划程序
✨ 守护进程模式 - 持续运行，自动定时聚合
✨ 可配置间隔 - 灵活调整聚合频率（5-60分钟）
✨ 完全重建策略 - 数据准确，无累积误差
✨ 自动创建表 - 首次运行自动创建统计表
✨ 错误处理 - 优雅处理各种异常情况
✨ 详细日志 - 记录所有操作，便于排查问题

【数据流程】

远端日志
  ↓ (HTTP POST)
logs/mozi/2025-10-31.txt
  ↓ (日志处理器)
botnet_nodes_mozi (原始节点表)
  ↓ (统计聚合器 - 每30分钟)
  ├─→ china_botnet_mozi (省市统计表)
  └─→ global_botnet_mozi (国家统计表)
      ↓
    前端展示

【聚合策略】

1. 完全重建（非增量更新）
   - 每次清空统计表
   - 从节点表完全重新统计
   - 保证数据准确性

2. 按需聚合
   - 如果节点表为空，跳过聚合
   - 如果表不存在，自动创建

3. 批量处理
   - 使用SQL GROUP BY高效聚合
   - 一次性写入所有统计数据

【文件清单】

新增文件：
  backend/stats_aggregator/
    ├── __init__.py                     # 模块初始化
    ├── aggregator.py                   # 核心程序（370行）
    ├── config.yaml                     # 配置文件
    └── README.md                       # 技术文档
  
  backend/
    ├── start_aggregator.bat            # Windows启动脚本
    ├── start_aggregator.sh             # Linux/Mac启动脚本
    ├── test_aggregator.py              # 测试脚本
    └── stats_aggregator.log            # 运行日志（自动生成）
  
  根目录/
    ├── start_all_services.bat          # Windows一键启动
    ├── start_all_services.sh           # Linux/Mac一键启动
    ├── stop_all_services.sh            # Linux/Mac停止脚本
    ├── 统计聚合器使用指南.md           # 使用指南
    └── 统计聚合器实现总结.txt          # 本文件

【验证步骤】

步骤1: 测试
  cd backend
  python test_aggregator.py

步骤2: 单次聚合（看看效果）
  python stats_aggregator/aggregator.py once

步骤3: 检查数据库
  mysql -u root -p123456 botnet
  SELECT * FROM china_botnet_mozi LIMIT 10;
  SELECT * FROM global_botnet_mozi LIMIT 10;

步骤4: 测试API
  curl http://localhost:8000/api/province-amounts

步骤5: 查看前端
  浏览器访问前端，按Ctrl+F5强制刷新
  检查地图和左侧统计是否显示数据

步骤6: 启动守护进程
  python stats_aggregator/aggregator.py daemon 30
  
  或使用启动脚本：
  start_aggregator.bat (Windows)
  ./start_aggregator.sh (Linux/Mac)

【配置说明】

默认配置（backend/stats_aggregator/aggregator.py）：
  - 聚合间隔: 30分钟（可通过命令行参数修改）
  - 僵尸网络类型: ['asruex', 'mozi', 'andromeda', 'moobot', 'ramnit', 'leethozer']
  - 日志级别: INFO
  - 日志文件: backend/stats_aggregator.log

数据库配置（backend/config.py）：
  DB_CONFIG = {
      "host": "localhost",
      "user": "root",
      "password": "123456",
      "database": "botnet"
  }

【日志文件】

查看实时日志：
  # Linux/Mac
  tail -f backend/stats_aggregator.log
  
  # Windows (PowerShell)
  Get-Content backend\stats_aggregator.log -Wait -Tail 50

日志内容示例：
  2025-10-31 10:00:00 - INFO - ============================================================
  2025-10-31 10:00:00 - INFO - 开始全量聚合统计数据
  2025-10-31 10:00:00 - INFO - ============================================================
  2025-10-31 10:00:01 - INFO - [mozi] 开始聚合统计数据...
  2025-10-31 10:00:01 - INFO - [mozi] 节点表共有 150 条记录
  2025-10-31 10:00:02 - INFO - ✅ [mozi] 聚合完成：节点 150 → 中国统计 25 条，全球统计 8 条
  2025-10-31 10:00:03 - INFO - [asruex] 开始聚合统计数据...
  ...
  2025-10-31 10:00:15 - INFO - ============================================================
  2025-10-31 10:00:15 - INFO - 聚合完成！耗时: 15.23秒
  2025-10-31 10:00:15 - INFO - 成功: 6/6
  2025-10-31 10:00:15 - INFO - 总计: 中国统计 156 条，全球统计 89 条
  2025-10-31 10:00:15 - INFO - ============================================================
  2025-10-31 10:00:15 - INFO - 
  2025-10-31 10:00:15 - INFO - ⏰ 下次聚合时间: 2025-10-31 10:30:15 (等待 30 分钟)

【常见问题】

Q1: 聚合器启动后没反应？
A1: 查看日志文件 backend/stats_aggregator.log

Q2: 统计表没有数据？
A2: 检查节点表是否有数据：SELECT COUNT(*) FROM botnet_nodes_mozi;

Q3: 想立即看到效果？
A3: 手动执行一次：python stats_aggregator/aggregator.py once

Q4: 如何修改聚合间隔？
A4: 启动时指定分钟数：python stats_aggregator/aggregator.py daemon 5

Q5: 前端还是没数据？
A5: 按顺序检查：
    1. 节点表有数据吗？
    2. 统计表有数据吗？
    3. API返回数据吗？curl http://localhost:8000/api/province-amounts
    4. 浏览器缓存清理了吗？Ctrl+F5
    5. 后端配置正确吗？检查 backend/main.py 是否从 config 导入了 DB_CONFIG

【技术亮点】

1. 守护进程设计
   - 使用 time.sleep() 实现定时循环
   - 捕获 KeyboardInterrupt 优雅退出
   - 异常处理保证服务稳定

2. 完全重建策略
   - 简单可靠，不会累积误差
   - 使用 DELETE + INSERT 而非 UPDATE
   - 数据一致性有保障

3. 自动化部署
   - 无需手动设置定时任务
   - 代码内置定时器
   - 一键启动所有服务

4. 灵活配置
   - 命令行参数控制间隔
   - 支持单次/守护进程模式
   - 可指定僵尸网络类型

5. 完善的测试
   - 测试脚本覆盖所有功能
   - 详细的输出信息
   - 清晰的错误提示

【性能考虑】

当前数据量（假设）：
  - 每个僵尸网络: 1000-10000 节点
  - 聚合耗时: < 5秒/类型
  - 总耗时: < 30秒（6个类型）

扩展能力：
  - 10万节点: 聚合间隔建议30-60分钟
  - 100万节点: 考虑在低峰期聚合
  - 1000万节点: 需要分区表和增量更新

【下一步】

1. 启动测试:
   python backend/test_aggregator.py

2. 单次聚合看效果:
   python backend/stats_aggregator/aggregator.py once

3. 启动守护进程:
   start_aggregator.bat (Windows)
   ./start_aggregator.sh (Linux/Mac)

4. 或一键启动所有服务:
   start_all_services.bat (Windows)
   ./start_all_services.sh (Linux/Mac)

5. 访问前端验证数据显示

【支持的僵尸网络】

✅ asruex
✅ mozi
✅ andromeda
✅ moobot
✅ ramnit
✅ leethozer

【实现完成时间】
2025年10月31日

【实现方式】
方案一：定时聚合（完全重建）
- 代码内置定时器，每30分钟自动聚合
- 无需手动设置系统定时任务
- 适合自动化运行的生产环境

================================================================================
实现完成！现在可以开始测试了。
祝使用愉快！🎉
================================================================================

