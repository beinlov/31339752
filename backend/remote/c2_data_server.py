#!/usr/bin/env python3
"""
C2ç«¯æ•°æ®æœåŠ¡å™¨ - ç‹¬ç«‹ç‰ˆæœ¬ï¼ˆä¸ä¾èµ– remote_uploader.pyï¼‰
æä¾›HTTPæ¥å£ä¾›æœåŠ¡å™¨æ‹‰å–æ•°æ®

éƒ¨ç½²ï¼šåªéœ€è¦è¿™ä¸€ä¸ªæ–‡ä»¶ + config.json
"""

import asyncio
import json
import logging
import os
import re
import sys
import time
import sqlite3
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from aiohttp import web
import aiofiles

# ============================================================
# å…ˆåˆå§‹åŒ–æ—¥å¿—
# ============================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s'
)
logger = logging.getLogger(__name__)

# ============================================================
# ç„¶ååŠ è½½é…ç½®
# ============================================================

def load_config(config_file: str = "config.json") -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    default_config = {
        "botnet": {
            "botnet_type": "ramnit",
            "log_dir": "/home/ubuntu/logs",
            "log_file_pattern": "ramnit_{datetime}.log"
        }
    }
    
    if os.path.exists(config_file):
        try:
            with open(config_file, 'r') as f:
                config = json.load(f)
                logger.info(f"åŠ è½½é…ç½®æ–‡ä»¶: {config_file}")
                return config
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return default_config
    else:
        logger.warning(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        return default_config

# åŠ è½½é…ç½®
CONFIG = load_config()
BOTNET_TYPE = CONFIG["botnet"]["botnet_type"]
LOG_DIR = Path(CONFIG["botnet"]["log_dir"])
LOG_FILE_PATTERN = CONFIG["botnet"]["log_file_pattern"]

# HTTPæœåŠ¡å™¨é…ç½®
HTTP_HOST = os.environ.get("C2_HTTP_HOST", CONFIG.get("http_server", {}).get("host", "0.0.0.0"))
HTTP_PORT = int(os.environ.get("C2_HTTP_PORT", CONFIG.get("http_server", {}).get("port", 8888)))
# ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œå…¶æ¬¡ä»config.jsonï¼Œæœ€åä½¿ç”¨é»˜è®¤å€¼
API_KEY = os.environ.get("C2_API_KEY", CONFIG.get("http_server", {}).get("api_key", "KiypG4zWLXqnREqGPH8L2Oh9ybvi6Yh4"))

# æ•°æ®ç¼“å­˜é…ç½®ï¼ˆæ”¹ç”¨SQLiteæŒä¹…åŒ–ï¼‰
CACHE_CONFIG = CONFIG.get("cache", {})
MAX_CACHED_RECORDS = CACHE_CONFIG.get("max_cached_records", 10000)
CACHE_DB_FILE = CACHE_CONFIG.get("db_file", "/tmp/c2_data_cache.db")
CACHE_RETENTION_DAYS = CACHE_CONFIG.get("retention_days", 7)

# åºåˆ—IDé…ç½®ï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼‰
SEQ_ID_START = 1  # åºåˆ—IDèµ·å§‹å€¼

# IPè§£ææ­£åˆ™
IP_REGEX = re.compile(r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b')

# ============================================================
# èƒŒå‹æ§åˆ¶å™¨ï¼ˆBackpressure Controllerï¼‰
# ============================================================

class BackpressureController:
    """
    èƒŒå‹æ§åˆ¶å™¨ - æ ¹æ®ç¼“å­˜é‡åŠ¨æ€è°ƒæ•´è¯»å–è¡Œä¸º
    
    ç­–ç•¥ï¼š
    1. ç¼“å­˜ >= é«˜æ°´ä½çº¿ â†’ æš‚åœè¯»å–ï¼ˆè¿”å›0ï¼‰
    2. ç¼“å­˜ <= ä½æ°´ä½çº¿ â†’ å…¨é€Ÿè¯»å–ï¼ˆè¿”å›max_batchï¼‰
    3. ç¼“å­˜åœ¨ä¸­é—´       â†’ çº¿æ€§ç¼©æ”¾è¯»å–é‡
    """
    
    def __init__(self, config: Dict):
        """
        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«ï¼š
                - max_cached_records: æœ€å¤§ç¼“å­˜è®°å½•æ•°ï¼ˆç¡¬é™åˆ¶ï¼‰
                - high_watermark: é«˜æ°´ä½çº¿ï¼ˆæš‚åœè¯»å–é˜ˆå€¼ï¼‰
                - low_watermark: ä½æ°´ä½çº¿ï¼ˆå…¨é€Ÿè¯»å–é˜ˆå€¼ï¼‰
                - read_batch_size: åŸºç¡€è¯»å–æ‰¹é‡å¤§å°
                - adaptive_read: æ˜¯å¦å¯ç”¨è‡ªé€‚åº”è¯»å–ï¼ˆé»˜è®¤Trueï¼‰
        """
        self.max_cached = config.get('max_cached_records', 10000)
        self.high_watermark = config.get('high_watermark', int(self.max_cached * 0.8))
        self.low_watermark = config.get('low_watermark', int(self.max_cached * 0.2))
        self.read_batch_size = config.get('read_batch_size', 5000)
        self.adaptive_read = config.get('adaptive_read', True)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_paused = 0
        self.total_throttled = 0
        self.total_full_speed = 0
        
        logger.info(f"èƒŒå‹æ§åˆ¶å™¨åˆå§‹åŒ–:")
        logger.info(f"  - æœ€å¤§ç¼“å­˜: {self.max_cached} æ¡")
        logger.info(f"  - é«˜æ°´ä½çº¿: {self.high_watermark} æ¡ ({self.high_watermark/self.max_cached*100:.0f}%)")
        logger.info(f"  - ä½æ°´ä½çº¿: {self.low_watermark} æ¡ ({self.low_watermark/self.max_cached*100:.0f}%)")
        logger.info(f"  - åŸºç¡€è¯»å–é‡: {self.read_batch_size} æ¡")
        logger.info(f"  - è‡ªé€‚åº”æ¨¡å¼: {'å¯ç”¨' if self.adaptive_read else 'ç¦ç”¨'}")
    
    def calculate_read_size(self, current_cached: int) -> Tuple[int, str]:
        """
        è®¡ç®—æœ¬æ¬¡åº”è¯¥è¯»å–çš„è®°å½•æ•°
        
        Args:
            current_cached: å½“å‰ç¼“å­˜çš„è®°å½•æ•°
        
        Returns:
            (read_size, reason) - è¯»å–æ•°é‡å’ŒåŸå› 
        """
        if not self.adaptive_read:
            # ç®€å•æ¨¡å¼ï¼šè¶…è¿‡æœ€å¤§å€¼å°±ä¸è¯»
            if current_cached >= self.max_cached:
                self.total_paused += 1
                return 0, f"ç¼“å­˜å·²æ»¡({current_cached}/{self.max_cached})"
            else:
                self.total_full_speed += 1
                return self.read_batch_size, "æ­£å¸¸è¯»å–"
        
        # è‡ªé€‚åº”æ¨¡å¼
        if current_cached >= self.high_watermark:
            # é«˜æ°´ä½ï¼šæš‚åœè¯»å–
            self.total_paused += 1
            return 0, f"èƒŒå‹æš‚åœ({current_cached}/{self.high_watermark})"
        
        elif current_cached <= self.low_watermark:
            # ä½æ°´ä½ï¼šå…¨é€Ÿè¯»å–
            self.total_full_speed += 1
            return self.read_batch_size, f"å…¨é€Ÿè¯»å–({current_cached}/{self.low_watermark})"
        
        else:
            # ä¸­é—´æ°´ä½ï¼šçº¿æ€§ç¼©æ”¾
            # è®¡ç®—ç¼©æ”¾å› å­ï¼šä»ä½æ°´ä½çš„100%åˆ°é«˜æ°´ä½çš„0%
            available_range = self.high_watermark - self.low_watermark
            current_offset = current_cached - self.low_watermark
            scale_factor = 1.0 - (current_offset / available_range)
            
            read_size = int(self.read_batch_size * scale_factor)
            read_size = max(100, read_size)  # è‡³å°‘è¯»100æ¡
            
            self.total_throttled += 1
            return read_size, f"èŠ‚æµè¯»å–(ç¼“å­˜{current_cached}æ¡,é€Ÿåº¦{scale_factor*100:.0f}%)"
    
    def should_skip_read(self, current_cached: int) -> Tuple[bool, str]:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥è·³è¿‡æœ¬æ¬¡è¯»å–
        
        Returns:
            (should_skip, reason)
        """
        read_size, reason = self.calculate_read_size(current_cached)
        return read_size == 0, reason
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        total_decisions = self.total_paused + self.total_throttled + self.total_full_speed
        
        if total_decisions == 0:
            return {
                'total_decisions': 0,
                'paused_rate': '0%',
                'throttled_rate': '0%',
                'full_speed_rate': '0%'
            }
        
        return {
            'total_decisions': total_decisions,
            'paused_count': self.total_paused,
            'throttled_count': self.total_throttled,
            'full_speed_count': self.total_full_speed,
            'paused_rate': f"{self.total_paused/total_decisions*100:.1f}%",
            'throttled_rate': f"{self.total_throttled/total_decisions*100:.1f}%",
            'full_speed_rate': f"{self.total_full_speed/total_decisions*100:.1f}%"
        }
    
    def log_stats(self):
        """è¾“å‡ºç»Ÿè®¡ä¿¡æ¯"""
        stats = self.get_stats()
        if stats['total_decisions'] > 0:
            logger.info(f"ğŸ“Š èƒŒå‹ç»Ÿè®¡: æš‚åœ{stats['paused_rate']}, èŠ‚æµ{stats['throttled_rate']}, å…¨é€Ÿ{stats['full_speed_rate']}")

# ============================================================
# é€šç”¨å·¥å…·ç±»
# ============================================================

class LogReader:
    """æ—¥å¿—æ–‡ä»¶è¯»å–å™¨"""
    
    def __init__(self, log_dir: Path, file_pattern: str):
        self.log_dir = log_dir
        self.file_pattern = file_pattern
    
    async def get_available_log_files(self, days_back: int = None, include_current: bool = True) -> List[Tuple[datetime, Path]]:
        """
        è·å–å¯ç”¨çš„æ—¥å¿—æ–‡ä»¶åˆ—è¡¨
        
        Args:
            days_back: å›æº¯å¤©æ•°ï¼ŒNoneè¡¨ç¤ºæ— é™å›æº¯ï¼ˆè¯»å–æ‰€æœ‰å†å²æ–‡ä»¶ï¼‰
            include_current: æ˜¯å¦åŒ…å«å½“å‰çš„æ–‡ä»¶ï¼ˆæ­£åœ¨å†™å…¥çš„ï¼‰
        """
        files = []
        now = datetime.now()
        current_hour = now.replace(minute=0, second=0, microsecond=0)
        
        # ç¡®å®šæœ€æ—©æ—¶é—´
        if days_back is None:
            # æ— é™å›æº¯ï¼šä½¿ç”¨ä¸€ä¸ªå¾ˆæ—©çš„æ—¶é—´
            earliest_time = datetime(2000, 1, 1)
            logger.info("æ— é™å›æº¯æ¨¡å¼ï¼šå°†è¯»å–æ‰€æœ‰å†å²æ—¥å¿—æ–‡ä»¶")
        else:
            earliest_time = now - timedelta(days=days_back)
            logger.info(f"é™åˆ¶å›æº¯æ¨¡å¼ï¼šå›æº¯{days_back}å¤©")
        
        if not self.log_dir.exists():
            logger.warning(f"æ—¥å¿—ç›®å½•ä¸å­˜åœ¨: {self.log_dir}")
            return files
        
        # æ”¯æŒ {date} æ ¼å¼ï¼ˆæŒ‰å¤©ï¼‰- æ¨è
        if '{date}' in self.file_pattern:
            pattern_parts = self.file_pattern.split('{date}')
            if len(pattern_parts) == 2:
                prefix, suffix = pattern_parts
                glob_pattern = f"{prefix}*{suffix}"
                
                for file_path in self.log_dir.glob(glob_pattern):
                    if file_path.is_file():
                        try:
                            filename = file_path.name
                            date_str = filename[len(prefix):-len(suffix)] if suffix else filename[len(prefix):]
                            
                            # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼
                            file_date = None
                            for date_format in ['%Y%m%d', '%Y-%m-%d']:
                                try:
                                    file_date = datetime.strptime(date_str, date_format)
                                    break
                                except ValueError:
                                    continue
                            
                            if file_date is None:
                                logger.debug(f"æ— æ³•è§£ææ–‡ä»¶æ—¥æœŸ: {filename}, æ—¥æœŸå­—ç¬¦ä¸²: {date_str}")
                                continue
                            
                            # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦åŒ…å«å½“å¤©çš„æ–‡ä»¶
                            current_day = now.replace(hour=0, minute=0, second=0, microsecond=0)
                            
                            if include_current:
                                # å®æ—¶æ¨¡å¼ï¼šåŒ…æ‹¬å½“å¤©çš„æ–‡ä»¶
                                if earliest_time.date() <= file_date.date() <= now.date():
                                    files.append((file_date, file_path))
                                    logger.debug(f"æ·»åŠ æ—¥å¿—æ–‡ä»¶: {filename}, æ—¥æœŸ: {file_date.date()}")
                            else:
                                # ä¿å®ˆæ¨¡å¼ï¼šåªå¤„ç†æ˜¨å¤©åŠæ›´æ—©çš„æ–‡ä»¶
                                if earliest_time.date() <= file_date.date() < current_day.date():
                                    files.append((file_date, file_path))
                                    logger.debug(f"æ·»åŠ æ—¥å¿—æ–‡ä»¶: {filename}, æ—¥æœŸ: {file_date.date()}")
                        except Exception as e:
                            logger.warning(f"å¤„ç†æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
                            continue
        
        # æ”¯æŒ {datetime} æ ¼å¼ï¼ˆæŒ‰å°æ—¶ï¼‰- å…¼å®¹æ—§æ ¼å¼
        elif '{datetime}' in self.file_pattern:
            pattern_parts = self.file_pattern.split('{datetime}')
            if len(pattern_parts) == 2:
                prefix, suffix = pattern_parts
                glob_pattern = f"{prefix}*{suffix}"
                
                for file_path in self.log_dir.glob(glob_pattern):
                    if file_path.is_file():
                        try:
                            filename = file_path.name
                            datetime_str = filename[len(prefix):-len(suffix)] if suffix else filename[len(prefix):]
                            file_datetime = datetime.strptime(datetime_str, '%Y-%m-%d_%H')
                            
                            # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦åŒ…å«å½“å‰å°æ—¶çš„æ–‡ä»¶
                            if include_current:
                                # å®æ—¶æ¨¡å¼ï¼šåŒ…å«å½“å‰å°æ—¶çš„æ–‡ä»¶
                                if earliest_time <= file_datetime <= now:
                                    files.append((file_datetime, file_path))
                            else:
                                # ä¿å®ˆæ¨¡å¼ï¼šåªå¤„ç†ä¸Šä¸€å°æ—¶åŠæ›´æ—©çš„æ–‡ä»¶
                                current_hour = now.replace(minute=0, second=0, microsecond=0)
                                if earliest_time <= file_datetime < current_hour:
                                    files.append((file_datetime, file_path))
                        except ValueError as e:
                            logger.debug(f"æ— æ³•è§£ææ–‡ä»¶æ—¥æœŸ: {filename}, é”™è¯¯: {e}")
                            continue
        
        return sorted(files, key=lambda x: x[0])


class IPProcessor:
    """IPåœ°å€å¤„ç†å™¨"""
    
    def __init__(self, botnet_type: str):
        self.botnet_type = botnet_type
    
    def normalize_ip(self, ip: str) -> Optional[str]:
        """è§„èŒƒåŒ–IPåœ°å€ï¼ˆå»é™¤å‰å¯¼é›¶ï¼‰å¹¶éªŒè¯"""
        try:
            parts = ip.split('.')
            if len(parts) != 4:
                return None
            
            normalized_parts = []
            for part in parts:
                num = int(part)  # è‡ªåŠ¨å»é™¤å‰å¯¼é›¶
                if num < 0 or num > 255:
                    return None
                normalized_parts.append(str(num))
            
            normalized_ip = '.'.join(normalized_parts)
            
            # è¿‡æ»¤ç§æœ‰IP
            if normalized_ip.startswith(('127.', '10.', '192.168.', '169.254.')):
                return None
            if normalized_ip.startswith('172.'):
                second_octet = int(normalized_parts[1])
                if 16 <= second_octet <= 31:
                    return None
            
            return normalized_ip
        except:
            return None
    
    def extract_ip_and_timestamp_from_line(self, line: str, file_path: Path = None) -> Optional[Dict]:
        """ä»æ—¥å¿—è¡Œæå–IPå’Œæ—¶é—´æˆ³"""
        try:
            # å°è¯•è§£ææ—¶é—´æˆ³
            log_time = None
            timestamp_source = "parsed"
            
            # å¸¸è§æ—¶é—´æ ¼å¼
            time_formats = [
                '%Y-%m-%d %H:%M:%S',
                '%Y/%m/%d %H:%M:%S',
                '%Y-%m-%d %H:%M:%S.%f',
            ]
            
            # ä»è¡Œé¦–æå–æ—¶é—´æˆ³
            if len(line) >= 19:
                potential_timestamp = line[:19]
                for fmt in time_formats:
                    try:
                        log_time = datetime.strptime(potential_timestamp, fmt)
                        break
                    except:
                        continue
            
            # å¦‚æœæ— æ³•è§£æï¼Œä½¿ç”¨æ–‡ä»¶ä¿®æ”¹æ—¶é—´
            if not log_time and file_path and file_path.exists():
                log_time = datetime.fromtimestamp(file_path.stat().st_mtime)
                timestamp_source = "file_mtime"
            
            # æœ€åä½¿ç”¨å½“å‰æ—¶é—´
            if not log_time:
                log_time = datetime.now()
                timestamp_source = "fallback_now"
            
            # æå–IPåœ°å€
            ips = IP_REGEX.findall(line)
            if ips:
                for ip in ips:
                    normalized_ip = self.normalize_ip(ip)
                    if normalized_ip:
                        return {
                            'ip': normalized_ip,
                            'raw_ip': ip if ip != normalized_ip else None,
                            'timestamp': log_time.isoformat(),
                            'timestamp_source': timestamp_source,
                            'date': log_time.strftime('%Y-%m-%d'),
                            'log_hour': log_time.strftime('%Y-%m-%d_%H'),
                            'botnet_type': self.botnet_type
                        }
            
            return None
        except Exception as e:
            return None


# ============================================================
# æ•°æ®ç¼“å­˜ç®¡ç†å™¨
# ============================================================

class DataCache:
    """æ•°æ®ç¼“å­˜ç®¡ç†å™¨ - ä½¿ç”¨SQLiteæŒä¹…åŒ–å­˜å‚¨"""
    
    def __init__(self, db_file: str = CACHE_DB_FILE):
        self.db_file = db_file
        self.conn = None
        self.total_generated = 0
        self.total_pulled = 0
        self.last_seq_id = 0  # åˆå§‹åŒ–åºåˆ—ID
        self.init_database()
        self.load_stats()
    
    def init_database(self):
        """åˆå§‹åŒ–SQLiteæ•°æ®åº“"""
        try:
            self.conn = sqlite3.connect(self.db_file, check_same_thread=False)
            self.conn.row_factory = sqlite3.Row
            
            # åˆ›å»ºè¡¨ç»“æ„ï¼ˆæ–°å¢seq_idå­—æ®µï¼‰
            self.conn.execute('''
                CREATE TABLE IF NOT EXISTS cache (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    seq_id INTEGER NOT NULL UNIQUE,
                    ip TEXT NOT NULL,
                    timestamp TEXT NOT NULL,
                    data TEXT NOT NULL,
                    pulled INTEGER DEFAULT 0,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                    pulled_at TEXT,
                    UNIQUE(ip, timestamp)
                )
            ''')
            
            # åˆ›å»ºç´¢å¼•ä»¥æé«˜æŸ¥è¯¢æ€§èƒ½
            self.conn.execute('CREATE INDEX IF NOT EXISTS idx_seq_id ON cache(seq_id)')
            self.conn.execute('CREATE INDEX IF NOT EXISTS idx_pulled ON cache(pulled)')
            self.conn.execute('CREATE INDEX IF NOT EXISTS idx_created_at ON cache(created_at)')
            self.conn.execute('CREATE INDEX IF NOT EXISTS idx_timestamp ON cache(timestamp)')
            
            # åˆ›å»ºç»Ÿè®¡è¡¨ï¼ˆæ–°å¢last_seq_idå­—æ®µï¼‰
            self.conn.execute('''
                CREATE TABLE IF NOT EXISTS stats (
                    id INTEGER PRIMARY KEY CHECK (id = 1),
                    total_generated INTEGER DEFAULT 0,
                    total_pulled INTEGER DEFAULT 0,
                    last_seq_id INTEGER DEFAULT 0,
                    updated_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # åˆå§‹åŒ–ç»Ÿè®¡
            self.conn.execute('INSERT OR IGNORE INTO stats (id, total_generated, total_pulled, last_seq_id) VALUES (1, 0, 0, 0)')
            self.conn.commit()
            
            logger.info(f"SQLiteæ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ: {self.db_file}")
        except Exception as e:
            logger.error(f"SQLiteæ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def load_stats(self):
        """åŠ è½½ç»Ÿè®¡æ•°æ®"""
        try:
            cursor = self.conn.execute('SELECT * FROM stats WHERE id = 1')
            row = cursor.fetchone()
            if row:
                self.total_generated = row['total_generated']
                self.total_pulled = row['total_pulled']
                # sqlite3.Rowä¸æ”¯æŒ.get()æ–¹æ³•ï¼Œä½¿ç”¨try-except
                try:
                    self.last_seq_id = row['last_seq_id']
                except (KeyError, IndexError):
                    self.last_seq_id = 0
            
            # ç»Ÿè®¡æœªæ‹‰å–çš„è®°å½•æ•°
            cursor = self.conn.execute('SELECT COUNT(*) as count FROM cache WHERE pulled = 0')
            unpulled_count = cursor.fetchone()['count']
            
            logger.info(f"åŠ è½½ç¼“å­˜: {unpulled_count} æ¡æœªæ‹‰å–è®°å½•")
        except Exception as e:
            logger.error(f"åŠ è½½ç»Ÿè®¡æ•°æ®å¤±è´¥: {e}")
    
    def save_stats(self):
        """ä¿å­˜ç»Ÿè®¡æ•°æ®"""
        try:
            self.conn.execute('''
                UPDATE stats SET 
                    total_generated = ?,
                    total_pulled = ?,
                    last_seq_id = ?,
                    updated_at = CURRENT_TIMESTAMP
                WHERE id = 1
            ''', (self.total_generated, self.total_pulled, self.last_seq_id))
            self.conn.commit()
        except Exception as e:
            logger.error(f"ä¿å­˜ç»Ÿè®¡æ•°æ®å¤±è´¥: {e}")
    
    def add_records(self, records: List[Dict]):
        """æ·»åŠ æ–°è®°å½•ï¼ˆä½¿ç”¨æ‰¹é‡æ’å…¥ï¼Œè‡ªåŠ¨åˆ†é…seq_idï¼‰"""
        try:
            added_count = 0
            duplicate_count = 0
            for record in records:
                try:
                    # è‡ªåŠ¨åˆ†é…seq_id
                    self.last_seq_id += 1
                    seq_id = self.last_seq_id
                    
                    # åœ¨æ•°æ®ä¸­æ·»åŠ seq_id
                    record['seq_id'] = seq_id
                    
                    self.conn.execute('''
                        INSERT INTO cache (seq_id, ip, timestamp, data, pulled)
                        VALUES (?, ?, ?, ?, 0)
                    ''', (seq_id, record['ip'], record['timestamp'], json.dumps(record)))
                    added_count += 1
                except sqlite3.IntegrityError:
                    # é‡å¤è®°å½•ï¼Œè·³è¿‡ï¼ˆä½†seq_idå·²å¢åŠ ï¼Œä¿è¯å•è°ƒï¼‰
                    duplicate_count += 1
            
            self.conn.commit()
            self.total_generated += added_count
            self.save_stats()
            
            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            if duplicate_count > 0:
                logger.info(f"æ•°æ®å†™å…¥: æ–°å¢ {added_count} æ¡ï¼Œé‡å¤ {duplicate_count} æ¡")
            
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§ç¼“å­˜ï¼Œå¦‚æœè¶…è¿‡åˆ™è­¦å‘Š
            cursor = self.conn.execute('SELECT COUNT(*) as count FROM cache WHERE pulled = 0')
            unpulled_count = cursor.fetchone()['count']
            if unpulled_count > MAX_CACHED_RECORDS:
                logger.warning(f"ç¼“å­˜å·²è¶…é™ï¼š{unpulled_count} æ¡ï¼Œè¯·å°½å¿«æ‹‰å–")
            
        except Exception as e:
            logger.error(f"æ·»åŠ è®°å½•å¤±è´¥: {e}")
            self.conn.rollback()
    
    def get_records(self, count: int = 1000, since_seq: Optional[int] = None, since_ts: Optional[str] = None) -> List[Dict]:
        """
        è·å–æœªæ‹‰å–çš„è®°å½•ï¼ˆæ”¯æŒåºåˆ—ID + æ—¶é—´æˆ³åŒæ¸¸æ ‡ï¼‰
        
        Args:
            count: æœ€å¤§è¿”å›æ•°é‡
            since_seq: åªè¿”å›æ­¤åºåˆ—IDä¹‹åçš„è®°å½•ï¼ˆä¼˜å…ˆä½¿ç”¨ï¼‰
            since_ts: åªè¿”å›æ­¤æ—¶é—´ä¹‹åçš„è®°å½•ï¼ˆISOæ ¼å¼ï¼Œå¤‡ç”¨ï¼‰
        
        Returns:
            è®°å½•åˆ—è¡¨
        """
        try:
            # ä¼˜å…ˆä½¿ç”¨seq_idæ¸¸æ ‡ï¼ˆæœ€å¯é ï¼‰
            if since_seq is not None:
                cursor = self.conn.execute('''
                    SELECT id, seq_id, data FROM cache 
                    WHERE pulled = 0 AND seq_id > ?
                    ORDER BY seq_id ASC
                    LIMIT ?
                ''', (since_seq, count))
            # å…¶æ¬¡ä½¿ç”¨æ—¶é—´æˆ³æ¸¸æ ‡ï¼ˆä½¿ç”¨>=æ”¯æŒé‡å çª—å£ï¼‰
            elif since_ts:
                cursor = self.conn.execute('''
                    SELECT id, seq_id, data FROM cache 
                    WHERE pulled = 0 AND timestamp >= ?
                    ORDER BY seq_id ASC
                    LIMIT ?
                ''', (since_ts, count))
            # é»˜è®¤ä»å¤´å¼€å§‹æ‹‰å–
            else:
                cursor = self.conn.execute('''
                    SELECT id, seq_id, data FROM cache 
                    WHERE pulled = 0
                    ORDER BY seq_id ASC
                    LIMIT ?
                ''', (count,))
            
            rows = cursor.fetchall()
            records = []
            for row in rows:
                data = json.loads(row['data'])
                data['_cache_id'] = row['id']  # é™„åŠ ç¼“å­˜IDç”¨äºç¡®è®¤
                data['_seq_id'] = row['seq_id']  # é™„åŠ åºåˆ—ID
                records.append(data)
            
            return records
        except Exception as e:
            logger.error(f"è·å–è®°å½•å¤±è´¥: {e}")
            return []
    
    def confirm_pulled(self, count: int):
        """
        ç¡®è®¤å·²æ‹‰å–ï¼ˆæ ‡è®°ä¸ºå·²æ‹‰å–ï¼Œä¸ç«‹å³åˆ é™¤ï¼‰
        
        Args:
            count: ç¡®è®¤çš„è®°å½•æ•°é‡
        """
        try:
            if count > 0:
                # æ ‡è®°ä¸ºå·²æ‹‰å–
                self.conn.execute('''
                    UPDATE cache SET 
                        pulled = 1,
                        pulled_at = CURRENT_TIMESTAMP
                    WHERE id IN (
                        SELECT id FROM cache WHERE pulled = 0 
                        ORDER BY created_at ASC 
                        LIMIT ?
                    )
                ''', (count,))
                self.conn.commit()
                
                self.total_pulled += count
                self.save_stats()
                
                # ç»Ÿè®¡å‰©ä½™æœªæ‹‰å–è®°å½•
                cursor = self.conn.execute('SELECT COUNT(*) as count FROM cache WHERE pulled = 0')
                unpulled_count = cursor.fetchone()['count']
                
                logger.info(f"ç¡®è®¤æ‹‰å– {count} æ¡ï¼Œå‰©ä½™: {unpulled_count} æ¡")
                
                # æ¸…ç†æ—§çš„å·²æ‹‰å–è®°å½•ï¼ˆä¿ç•™7å¤©ï¼‰
                self.cleanup_old_records()
        except Exception as e:
            logger.error(f"ç¡®è®¤æ‹‰å–å¤±è´¥: {e}")
            self.conn.rollback()
    
    def cleanup_old_records(self):
        """æ¸…ç†è¿‡æœŸçš„å·²æ‹‰å–è®°å½•"""
        try:
            cursor = self.conn.execute('''
                DELETE FROM cache 
                WHERE pulled = 1 
                AND pulled_at < datetime('now', '-' || ? || ' days')
            ''', (CACHE_RETENTION_DAYS,))
            
            deleted_count = cursor.rowcount
            if deleted_count > 0:
                self.conn.commit()
                logger.info(f"æ¸…ç†äº† {deleted_count} æ¡è¿‡æœŸè®°å½•")
        except Exception as e:
            logger.error(f"æ¸…ç†è¿‡æœŸè®°å½•å¤±è´¥: {e}")
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        try:
            cursor = self.conn.execute('SELECT COUNT(*) as count FROM cache WHERE pulled = 0')
            cached_records = cursor.fetchone()['count']
            
            cursor = self.conn.execute('SELECT COUNT(*) as count FROM cache WHERE pulled = 1')
            pulled_records = cursor.fetchone()['count']
            
            return {
                'cached_records': cached_records,
                'pulled_records': pulled_records,
                'total_generated': self.total_generated,
                'total_pulled': self.total_pulled,
                'cache_full': cached_records >= MAX_CACHED_RECORDS
            }
        except Exception as e:
            logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {
                'cached_records': 0,
                'pulled_records': 0,
                'total_generated': self.total_generated,
                'total_pulled': self.total_pulled,
                'cache_full': False
            }
    
    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.conn:
            self.conn.close()
            logger.info("æ•°æ®åº“è¿æ¥å·²å…³é—­")


# å…¨å±€æ•°æ®ç¼“å­˜
data_cache = DataCache()

# ============================================================
# åå°æ—¥å¿—è¯»å–ä»»åŠ¡
# ============================================================

class BackgroundLogReader:
    """åå°æ—¥å¿—è¯»å–å™¨ï¼ˆå¸¦èƒŒå‹æ§åˆ¶ï¼‰"""
    
    def __init__(self, cache: DataCache):
        self.cache = cache
        self.log_reader = LogReader(LOG_DIR, LOG_FILE_PATTERN)
        self.ip_processor = IPProcessor(BOTNET_TYPE)
        self.running = False
        
        # æ–‡ä»¶ä½ç½®æŒä¹…åŒ–æ–‡ä»¶
        self.positions_file = '/tmp/c2_file_positions.json'
        self.file_positions = self._load_file_positions()
        
        # ä»é…ç½®è¯»å–å‚æ•°
        processing_config = CONFIG.get('processing', {})
        self.read_interval = processing_config.get('read_interval', 60)
        self.max_files_per_read = processing_config.get('max_files_per_read', 10)
        
        # åˆå§‹åŒ–èƒŒå‹æ§åˆ¶å™¨
        self.backpressure = BackpressureController(CACHE_CONFIG)
        self.stats_counter = 0  # ç”¨äºå®šæœŸè¾“å‡ºç»Ÿè®¡
        
        # åŠ è½½å›æº¯é…ç½®
        try:
            from config import C2_LOG_LOOKBACK_CONFIG
            self.lookback_config = C2_LOG_LOOKBACK_CONFIG
            logger.info(f"å›æº¯é…ç½®: {C2_LOG_LOOKBACK_CONFIG['mode']}")
        except ImportError:
            self.lookback_config = {'mode': 'unlimited', 'max_days': 90}
            logger.warning("æ— æ³•åŠ è½½å›æº¯é…ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼ï¼ˆæ— é™å›æº¯ï¼‰")
        
        logger.info(f"åˆå§‹åŒ–æ—¥å¿—è¯»å–å™¨:")
        logger.info(f"  - æ—¥å¿—ç›®å½•: {LOG_DIR}")
        logger.info(f"  - æ–‡ä»¶æ¨¡å¼: {LOG_FILE_PATTERN}")
        logger.info(f"  - åƒµå°¸ç½‘ç»œ: {BOTNET_TYPE}")
        logger.info(f"  - è¯»å–é—´éš”: {self.read_interval}ç§’")
        logger.info(f"  - ä½ç½®è®°å½•: {len(self.file_positions)} ä¸ªæ–‡ä»¶")
    
    def _load_file_positions(self) -> Dict[str, int]:
        """ä»æ–‡ä»¶åŠ è½½å·²è¯»å–ä½ç½®"""
        try:
            if os.path.exists(self.positions_file):
                with open(self.positions_file, 'r') as f:
                    positions = json.load(f)
                    logger.info(f"å·²åŠ è½½ {len(positions)} ä¸ªæ–‡ä»¶çš„è¯»å–ä½ç½®")
                    return positions
        except Exception as e:
            logger.warning(f"åŠ è½½æ–‡ä»¶ä½ç½®å¤±è´¥: {e}ï¼Œå°†ä»å¤´å¼€å§‹è¯»å–")
        return {}
    
    def _save_file_positions(self):
        """ä¿å­˜æ–‡ä»¶è¯»å–ä½ç½®"""
        try:
            with open(self.positions_file, 'w') as f:
                json.dump(self.file_positions, f, indent=2)
            logger.debug(f"å·²ä¿å­˜ {len(self.file_positions)} ä¸ªæ–‡ä»¶çš„è¯»å–ä½ç½®")
        except Exception as e:
            logger.error(f"ä¿å­˜æ–‡ä»¶ä½ç½®å¤±è´¥: {e}")
    
    async def run(self):
        """è¿è¡Œåå°è¯»å–ä»»åŠ¡"""
        self.running = True
        logger.info("åå°æ—¥å¿—è¯»å–ä»»åŠ¡å¯åŠ¨")
        
        while self.running:
            try:
                await self.read_logs()
                await asyncio.sleep(self.read_interval)
            except Exception as e:
                logger.error(f"è¯»å–æ—¥å¿—å¼‚å¸¸: {e}", exc_info=True)
                await asyncio.sleep(10)
    
    async def read_logs(self):
        """è¯»å–æ—¥å¿—æ–‡ä»¶ï¼ˆæ”¯æŒæ— é™å›æº¯+å¢é‡è¯»å–+èƒŒå‹æ§åˆ¶ï¼‰"""
        try:
            # ========== èƒŒå‹æ§åˆ¶æ£€æŸ¥ ==========
            stats = self.cache.get_stats()
            current_cached = stats['cached_records']
            
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡è¯»å–
            should_skip, reason = self.backpressure.should_skip_read(current_cached)
            
            if should_skip:
                logger.info(f"â¸ï¸  è·³è¿‡æœ¬æ¬¡è¯»å–: {reason}")
                return
            
            # è®¡ç®—æœ¬æ¬¡åº”è¯¥è¯»å–çš„æ•°é‡
            read_limit, read_reason = self.backpressure.calculate_read_size(current_cached)
            logger.info(f"ğŸ“– å¼€å§‹è¯»å–: {read_reason}, é™åˆ¶ {read_limit} æ¡")
            
            # ç¡®å®šå›æº¯å¤©æ•°
            if self.lookback_config['mode'] == 'unlimited':
                days_back = None  # æ— é™å›æº¯
            else:
                days_back = self.lookback_config['max_days']
            
            # è·å–å¯ç”¨çš„æ—¥å¿—æ–‡ä»¶ï¼ˆæŒ‰é…ç½®å›æº¯ï¼‰
            log_files = await self.log_reader.get_available_log_files(
                days_back=days_back,
                include_current=True
            )
            
            if not log_files:
                logger.debug("æ²¡æœ‰å¯è¯»å–çš„æ—¥å¿—æ–‡ä»¶")
                return
            
            logger.debug(f"å‘ç° {len(log_files)} ä¸ªå¯å¤„ç†çš„æ—¥å¿—æ–‡ä»¶")
            
            new_records = []
            files_read = 0
            
            # æŒ‰æ—¶é—´æ’åºï¼ˆä»æ—§åˆ°æ–°ï¼Œç¡®ä¿é¡ºåºè¯»å–ï¼‰
            log_files.sort(key=lambda x: x[0])
            
            for file_datetime, file_path in log_files:
                file_key = str(file_path)
                
                # è·å–ä¸Šæ¬¡è¯»å–çš„ä½ç½®
                last_position = self.file_positions.get(file_key, 0)
                
                logger.info(f"è¯»å–æ—¥å¿—æ–‡ä»¶: {file_path.name} (ä»ä½ç½® {last_position} ç»§ç»­)")
                
                try:
                    file_size = file_path.stat().st_size
                    
                    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²è¯»å®Œ
                    if last_position >= file_size and file_size > 0:
                        logger.debug(f"  æ–‡ä»¶å·²è¯»å®Œï¼Œè·³è¿‡: {file_path.name}")
                        files_read += 1
                        continue
                    
                    async with aiofiles.open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        # è·³åˆ°ä¸Šæ¬¡è¯»å–çš„ä½ç½®
                        await f.seek(last_position)
                        
                        line_count = 0
                        ip_count = 0
                        file_exhausted = False  # æ ‡è®°æ–‡ä»¶æ˜¯å¦è¯»å®Œ
                        
                        async for line in f:
                            line_count += 1
                            line = line.strip()
                            
                            if not line:
                                continue
                            
                            ip_data = self.ip_processor.extract_ip_and_timestamp_from_line(line, file_path)
                            
                            if ip_data:
                                new_records.append(ip_data)
                                ip_count += 1
                            
                            # ä½¿ç”¨èƒŒå‹æ§åˆ¶å™¨è®¡ç®—çš„é™åˆ¶é‡
                            if len(new_records) >= read_limit:
                                logger.debug(f"  è¾¾åˆ°è¯»å–é™åˆ¶({read_limit}æ¡)ï¼Œä¸­æ–­å½“å‰æ–‡ä»¶")
                                break
                        else:
                            # æ­£å¸¸éå†ç»“æŸï¼ˆæ²¡æœ‰breakï¼‰ï¼Œè¯´æ˜æ–‡ä»¶è¯»å®Œäº†
                            file_exhausted = True
                        
                        # ä¿å­˜å½“å‰è¯»å–ä½ç½®
                        current_position = await f.tell()
                        self.file_positions[file_key] = current_position
                        
                        if line_count > 0:
                            status = "è¯»å®Œ" if file_exhausted else "æœªå®Œ"
                            logger.info(f"  æ–‡ä»¶å¤„ç†: æ–°è¯»{line_count}è¡Œï¼Œæå–{ip_count}ä¸ªIPï¼Œä½ç½®:{last_position}â†’{current_position} [{status}]")
                        else:
                            logger.debug(f"  æ–‡ä»¶æ— æ–°å†…å®¹: {file_path.name}")
                    
                    files_read += 1
                    
                    # è¾¾åˆ°è¯»å–é™åˆ¶æ—¶åœæ­¢
                    if len(new_records) >= read_limit:
                        logger.info(f"è¾¾åˆ°è¯»å–é™åˆ¶: å·²è¯»{files_read}ä¸ªæ–‡ä»¶ï¼Œæå–{len(new_records)}æ¡")
                        break
                    
                    # è¾¾åˆ°æ–‡ä»¶æ•°é™åˆ¶æ—¶åœæ­¢
                    if files_read >= self.max_files_per_read:
                        logger.info(f"è¾¾åˆ°æ–‡ä»¶æ•°é™åˆ¶: å·²è¯»{files_read}ä¸ªæ–‡ä»¶")
                        break
                
                except Exception as e:
                    logger.error(f"è¯»å–æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
                    continue
            
            if new_records:
                self.cache.add_records(new_records)
                new_stats = self.cache.get_stats()
                logger.info(f"ğŸ“¦ æå– {len(new_records)} æ¡ â†’ å½“å‰ç¼“å­˜: {new_stats['cached_records']} æ¡")
                
                # ä¿å­˜æ–‡ä»¶ä½ç½®
                self._save_file_positions()
                
                # æ¯10æ¬¡è¾“å‡ºä¸€æ¬¡èƒŒå‹ç»Ÿè®¡
                self.stats_counter += 1
                if self.stats_counter % 10 == 0:
                    self.backpressure.log_stats()
            else:
                logger.debug("æœ¬æ¬¡æœªæå–åˆ°æ–°æ•°æ®")
            
            # æ¸…ç†è¿‡æœŸçš„æ–‡ä»¶ä½ç½®è®°å½•ï¼ˆä¿ç•™æœ€è¿‘30å¤©çš„ï¼‰
            if len(self.file_positions) > 100:
                recent_file_keys = {str(f[1]) for f in log_files[-100:]}
                self.file_positions = {k: v for k, v in self.file_positions.items() if k in recent_file_keys}
                self._save_file_positions()
        
        except Exception as e:
            logger.error(f"è¯»å–æ—¥å¿—å¤±è´¥: {e}", exc_info=True)
    
    def stop(self):
        """åœæ­¢åå°ä»»åŠ¡"""
        self.running = False


background_reader = None

# ============================================================
# HTTPæ¥å£
# ============================================================

def check_auth(request: web.Request) -> bool:
    """æ£€æŸ¥è®¤è¯"""
    auth_header = request.headers.get('Authorization', '')
    
    if auth_header.startswith('Bearer '):
        token = auth_header[7:]
        result = token == API_KEY
        if not result:
            logger.warning(f"è®¤è¯å¤±è´¥ (Bearer): æ”¶åˆ° {token[:6]}***, æœŸæœ› {API_KEY[:6]}***")
        return result
    
    api_key = request.headers.get('X-API-Key', '')
    result = api_key == API_KEY
    if not result:
        logger.warning(f"è®¤è¯å¤±è´¥ (X-API-Key): æ”¶åˆ° '{api_key[:6] if api_key else '(ç©º)'}***', æœŸæœ› '{API_KEY[:6]}***'")
        logger.warning(f"æ”¶åˆ°Keyé•¿åº¦: {len(api_key)}, æœŸæœ›é•¿åº¦: {len(API_KEY)}")
    return result


async def handle_pull(request: web.Request) -> web.Response:
    """
    å¤„ç†æ•°æ®æ‹‰å–è¯·æ±‚
    
    æ”¯æŒå‚æ•°ï¼š
    - limit: æœ€å¤§æ‹‰å–æ•°é‡ï¼ˆé»˜è®¤1000ï¼‰
    - since_seq: åªæ‹‰å–æ­¤åºåˆ—IDä¹‹åçš„æ•°æ®ï¼ˆä¼˜å…ˆä½¿ç”¨ï¼Œæœ€å¯é ï¼‰
    - since_ts: åªæ‹‰å–æ­¤æ—¶é—´ä¹‹åçš„æ•°æ®ï¼ˆå¤‡ç”¨ï¼Œæ”¯æŒé‡å çª—å£ï¼‰
    - since: å…¼å®¹æ—§ç‰ˆæœ¬ï¼Œç­‰åŒäºsince_ts
    - confirm: æ˜¯å¦è‡ªåŠ¨ç¡®è®¤åˆ é™¤ï¼ˆé»˜è®¤falseï¼Œä¸å»ºè®®ä½¿ç”¨trueï¼‰
    """
    if not check_auth(request):
        return web.json_response({'error': 'Unauthorized'}, status=401)
    
    try:
        params = request.rel_url.query
        
        # æ”¯æŒ limit å’Œ count å‚æ•°ï¼ˆå…¼å®¹æ€§ï¼‰
        limit = int(params.get('limit', params.get('count', 1000)))
        limit = min(limit, 5000)  # é™åˆ¶æœ€å¤§æ‹‰å–æ•°é‡
        
        # æ”¯æŒåºåˆ—IDæ¸¸æ ‡ï¼ˆä¼˜å…ˆï¼‰
        since_seq = params.get('since_seq', None)
        if since_seq is not None:
            since_seq = int(since_seq)
        
        # æ”¯æŒæ—¶é—´æˆ³æ¸¸æ ‡ï¼ˆå¤‡ç”¨ï¼‰
        since_ts = params.get('since_ts', params.get('since', None))
        
        # æ”¯æŒ confirm å‚æ•°ï¼ˆé»˜è®¤falseï¼Œæ¨èä½¿ç”¨ä¸¤é˜¶æ®µç¡®è®¤ï¼‰
        auto_confirm = params.get('confirm', 'false').lower() == 'true'
        
        # æ‹‰å–è®°å½•
        records = data_cache.get_records(limit, since_seq, since_ts)
        
        # å¦‚æœå¯ç”¨äº†è‡ªåŠ¨ç¡®è®¤ï¼ˆä¸æ¨èï¼‰
        if auto_confirm and records:
            data_cache.confirm_pulled(len(records))
            logger.info(f"æ‹‰å–è¯·æ±‚: è¿”å›å¹¶ç¡®è®¤ {len(records)} æ¡è®°å½•ï¼ˆè‡ªåŠ¨ç¡®è®¤æ¨¡å¼ï¼‰")
        else:
            logger.info(f"æ‹‰å–è¯·æ±‚: è¿”å› {len(records)} æ¡è®°å½•ï¼ˆæœªç¡®è®¤ï¼Œç­‰å¾…æœåŠ¡å™¨ç¡®è®¤ï¼‰")
        
        # è·å–å½“å‰æœ€å¤§seq_idï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼‰
        max_seq_id = max([r.get('_seq_id', 0) for r in records]) if records else 0
        
        response_data = {
            'success': True,
            'count': len(records),
            'data': records,  # ä½¿ç”¨ 'data' å­—æ®µï¼ˆåŒ¹é…æœ¬åœ°æ‹‰å–å™¨ï¼‰
            'records': records,  # ä¿ç•™å…¼å®¹æ€§
            'max_seq_id': max_seq_id,  # å½“å‰æ‰¹æ¬¡æœ€å¤§åºåˆ—ID
            'stats': data_cache.get_stats()
        }
        
        return web.json_response(response_data)
    
    except Exception as e:
        logger.error(f"æ‹‰å–å¤±è´¥: {e}")
        return web.json_response({'success': False, 'error': str(e)}, status=500)


async def handle_confirm(request: web.Request) -> web.Response:
    """å¤„ç†ç¡®è®¤æ‹‰å–è¯·æ±‚"""
    if not check_auth(request):
        return web.json_response({'error': 'Unauthorized'}, status=401)
    
    try:
        data = await request.json()
        count = data.get('count', 0)
        data_cache.confirm_pulled(count)
        
        return web.json_response({
            'success': True,
            'message': f'å·²ç¡®è®¤ {count} æ¡'
        })
    
    except Exception as e:
        return web.json_response({'error': str(e)}, status=500)


async def handle_stats(request: web.Request) -> web.Response:
    """å¤„ç†ç»Ÿè®¡è¯·æ±‚ï¼ˆåŒ…å«èƒŒå‹ç»Ÿè®¡ï¼‰"""
    stats = data_cache.get_stats()
    
    # æ·»åŠ èƒŒå‹ç»Ÿè®¡
    if background_reader and background_reader.backpressure:
        stats['backpressure_stats'] = background_reader.backpressure.get_stats()
    
    return web.json_response(stats)


async def handle_health(request: web.Request) -> web.Response:
    """å¥åº·æ£€æŸ¥"""
    return web.json_response({'status': 'ok', 'service': 'c2-data-server'})


# ============================================================
# åº”ç”¨å¯åŠ¨å’Œæ¸…ç†
# ============================================================

async def on_startup(app):
    """åº”ç”¨å¯åŠ¨æ—¶æ‰§è¡Œ"""
    global background_reader
    background_reader = BackgroundLogReader(data_cache)
    app['background_task'] = asyncio.create_task(background_reader.run())


async def on_cleanup(app):
    """åº”ç”¨æ¸…ç†æ—¶æ‰§è¡Œ"""
    global background_reader
    if background_reader:
        background_reader.stop()
    
    if 'background_task' in app:
        app['background_task'].cancel()
        try:
            await app['background_task']
        except asyncio.CancelledError:
            pass


def create_app() -> web.Application:
    """åˆ›å»ºåº”ç”¨"""
    app = web.Application()
    
    # æ”¯æŒä¸¤ç§è·¯å¾„ï¼š/api/pull å’Œ /pullï¼ˆå…¼å®¹æ€§ï¼‰
    app.router.add_get('/api/pull', handle_pull)
    app.router.add_get('/pull', handle_pull)
    app.router.add_post('/api/confirm', handle_confirm)
    app.router.add_post('/confirm', handle_confirm)
    app.router.add_get('/api/stats', handle_stats)
    app.router.add_get('/stats', handle_stats)
    app.router.add_get('/health', handle_health)
    app.router.add_get('/api/health', handle_health)
    
    app.on_startup.append(on_startup)
    app.on_cleanup.append(on_cleanup)
    
    return app


def main():
    """ä¸»å‡½æ•°"""
    logger.info("="*60)
    logger.info("C2ç«¯æ•°æ®æœåŠ¡å™¨å¯åŠ¨ï¼ˆç‹¬ç«‹ç‰ˆæœ¬ï¼‰")
    logger.info(f"HTTPæœåŠ¡: http://{HTTP_HOST}:{HTTP_PORT}")
    logger.info(f"æ—¥å¿—ç›®å½•: {LOG_DIR}")
    logger.info(f"åƒµå°¸ç½‘ç»œç±»å‹: {BOTNET_TYPE}")
    logger.info(f"API Keyé•¿åº¦: {len(API_KEY)}, å‰6ä½: {API_KEY[:6]}***")
    logger.info("="*60)
    
    if API_KEY == "your-secret-api-key-here":
        logger.warning("âš ï¸  è­¦å‘Š: ä½¿ç”¨é»˜è®¤API Keyï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ C2_API_KEY")
    elif API_KEY == "KiypG4zWLXqnREqGPH8L2Oh9ybvi6Yh4":
        logger.info("API Keyå·²æ­£ç¡®é…ç½®")
    
    app = create_app()
    web.run_app(app, host=HTTP_HOST, port=HTTP_PORT)


if __name__ == '__main__':
    main()
