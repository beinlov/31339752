#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¿œç«¯æ—¥å¿—ä¸Šä¼ è„šæœ¬
éƒ¨ç½²åœ¨è¿œç«¯èœœç½æœåŠ¡å™¨ä¸Šï¼Œå¼‚æ­¥è¯»å–æ¯æ—¥æ—¥å¿—æ–‡ä»¶ï¼Œå»é‡åä¸Šä¼ åˆ°æœ¬åœ°æœåŠ¡å™¨

æ¶æ„è®¾è®¡:
- LogReader: å¼‚æ­¥æ—¥å¿—è¯»å–å™¨ï¼Œè´Ÿè´£è¯»å–æ¯æ—¥æ—¥å¿—æ–‡ä»¶
- IPProcessor: IPå¤„ç†å™¨ï¼Œè´Ÿè´£è§£æã€å»é‡å’Œç¼“å­˜IPæ•°æ®
- RemoteUploader: ä¸Šä¼ å™¨ï¼Œè´Ÿè´£å°†å¤„ç†åçš„æ•°æ®ä¸Šä¼ åˆ°æœ¬åœ°æœåŠ¡å™¨
"""

import asyncio
import aiofiles
import aiohttp
import time
import os
import sys
import json
import re
from datetime import datetime, timedelta
from typing import List, Optional, Dict, Set, Tuple
import logging
from pathlib import Path
from collections import defaultdict

# ============================================================
# Configuration Loading
# ============================================================

def load_config(config_file: str = None) -> Dict:
    """
    Load configuration from JSON file
    If config file doesn't exist, use default configuration and create template
    """
    # Default configuration
    default_config = {
        "server": {
            "api_endpoint": "https://periotic-multifaced-christena.ngrok-free.dev",
            "api_key": "KiypG4zWLXqnREqGPH8L2Oh9ybvi6Yh4",
            "local_server_host": "your-local-server-ip",
            "local_server_port": 8000
        },
        "botnet": {
            "botnet_type": "ramnit",
            "log_dir": "/home/ubuntu",
            "log_file_pattern": "ramnit_{date}.log"
        },
        "processing": {
            "upload_interval": 300,
            "batch_size": 500,
            "max_retries": 3,
            "retry_delay": 30,
            "read_chunk_size": 8192
        },
        "server_check": {
            "check_interval": 60,
            "check_timeout": 10,
            "max_check_retries": 5
        },
        "files": {
            "state_file": "/tmp/uploader_state.json",
            "duplicate_cache_file": "/tmp/ip_cache.json",
            "offset_state_file": "/tmp/file_offsets.json"
        },
        "cache": {
            "expire_days": 7
        }
    }
    
    # Determine config file path
    if config_file is None:
        # Try to find config.json in the same directory as this script
        script_dir = Path(__file__).parent
        config_file = script_dir / "config.json"
    else:
        config_file = Path(config_file)
    
    # Load from file if exists
    if config_file.exists():
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                user_config = json.load(f)
                # Merge user config with default config
                for section, values in user_config.items():
                    if section in default_config:
                        default_config[section].update(values)
                    else:
                        default_config[section] = values
                print(f"Loaded configuration from: {config_file}")
        except Exception as e:
            print(f"Error loading config file: {e}")
            print("Using default configuration")
    else:
        # ğŸ”§ ä¿®å¤é—®é¢˜1ï¼šCreate template config file
        try:
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(default_config, f, indent=2, ensure_ascii=False)
            print("="*60)
            print(f"âœ“ å·²åˆ›å»ºé…ç½®æ–‡ä»¶æ¨¡æ¿: {config_file}")
            print("âš ï¸  è¯·å…ˆç¼–è¾‘é…ç½®æ–‡ä»¶ï¼Œç„¶åé‡æ–°è¿è¡Œç¨‹åºï¼")
            print("="*60)
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šæç¤ºåé€€å‡ºï¼Œé¿å…ç”¨é»˜è®¤é…ç½®è¿è¡Œ
            sys.exit(0)
        except Exception as e:
            print(f"Error creating config file: {e}")
    
    return default_config

# Load configuration
print("="*60)
print("æ­£åœ¨åŠ è½½é…ç½®...")
CONFIG = load_config()
print(f"é…ç½®åŠ è½½å®Œæˆï¼")
print(f"  APIç«¯ç‚¹: {CONFIG['server']['api_endpoint']}")
print(f"  åƒµå°¸ç½‘ç»œç±»å‹: {CONFIG['botnet']['botnet_type']}")
print(f"  æ—¥å¿—ç›®å½•: {CONFIG['botnet']['log_dir']}")
print(f"  æ‰¹æ¬¡å¤§å°: {CONFIG['processing']['batch_size']}")
print("="*60)

# Extract configuration values for easy access
API_ENDPOINT = CONFIG["server"]["api_endpoint"]
API_KEY = CONFIG["server"]["api_key"]
LOCAL_SERVER_HOST = CONFIG["server"]["local_server_host"]
LOCAL_SERVER_PORT = CONFIG["server"]["local_server_port"]

BOTNET_TYPE = CONFIG["botnet"]["botnet_type"]
LOG_DIR = CONFIG["botnet"]["log_dir"]
LOG_FILE_PATTERN = CONFIG["botnet"]["log_file_pattern"]

UPLOAD_INTERVAL = CONFIG["processing"]["upload_interval"]
BATCH_SIZE = CONFIG["processing"]["batch_size"]
MAX_RETRIES = CONFIG["processing"]["max_retries"]
RETRY_DELAY = CONFIG["processing"]["retry_delay"]
READ_CHUNK_SIZE = CONFIG["processing"]["read_chunk_size"]

SERVER_CHECK_INTERVAL = CONFIG["server_check"]["check_interval"]
SERVER_CHECK_TIMEOUT = CONFIG["server_check"]["check_timeout"]
MAX_SERVER_CHECK_RETRIES = CONFIG["server_check"]["max_check_retries"]

STATE_FILE = CONFIG["files"]["state_file"]
DUPLICATE_CACHE_FILE = CONFIG["files"]["duplicate_cache_file"]
OFFSET_STATE_FILE = CONFIG["files"]["offset_state_file"]
PENDING_QUEUE_FILE = "/tmp/pending_upload_queue.json"  # å¾…ä¸Šä¼ æ•°æ®æŒä¹…åŒ–é˜Ÿåˆ—

CACHE_EXPIRE_DAYS = CONFIG["cache"]["expire_days"]

# å†…å­˜é™åˆ¶é…ç½®
MAX_MEMORY_IPS = 10000  # å†…å­˜ä¸­æœ€å¤šä¿ç•™çš„IPæ•°é‡
FORCE_UPLOAD_THRESHOLD = 5000  # è¾¾åˆ°æ­¤æ•°é‡å¼ºåˆ¶ä¸Šä¼ 
MIN_UPLOAD_BATCH = 100  # æœ€å°ä¸Šä¼ æ‰¹æ¬¡

# æ–‡ä»¶æ‰«æç¼“å­˜é…ç½®
FILE_SCAN_CACHE_TTL = 300  # æ–‡ä»¶åˆ—è¡¨ç¼“å­˜æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤5åˆ†é’Ÿ

# ä¸å®Œæ•´è¡Œç¼“å­˜æ–‡ä»¶
INCOMPLETE_LINES_FILE = "/tmp/incomplete_lines.json"  # ä¿å­˜ä¸å®Œæ•´è¡Œçš„æ–‡ä»¶

# IP parsing regex
IP_REGEX = re.compile(r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b')

# ============================================================
# æ—¥å¿—é…ç½®
# ============================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('/tmp/remote_uploader.log')
    ]
)
logger = logging.getLogger(__name__)

# ============================================================
# æ ¸å¿ƒåŠŸèƒ½ç±»
# ============================================================

class LogReader:
    """å¼‚æ­¥æ—¥å¿—è¯»å–å™¨ï¼ˆæ”¹è¿›ç‰ˆï¼šæ”¯æŒç¼“å­˜å’Œä¸å®Œæ•´è¡Œå¤„ç†ï¼‰"""
    
    def __init__(self, log_dir: str, file_pattern: str):
        self.log_dir = Path(log_dir)
        self.file_pattern = file_pattern
        self.processed_files = set()
        
        # æ–‡ä»¶åˆ—è¡¨ç¼“å­˜
        self.file_cache = None
        self.file_cache_time = 0
        
        # ä¸å®Œæ•´è¡Œç¼“å­˜
        self.incomplete_lines = {}  # {file_path: incomplete_line}
        self.load_incomplete_lines()
        
    def get_log_file_path(self, date: datetime) -> Path:
        """è·å–æŒ‡å®šæ—¥æœŸçš„æ—¥å¿—æ–‡ä»¶è·¯å¾„"""
        date_str = date.strftime('%Y-%m-%d')
        filename = self.file_pattern.format(date=date_str)
        return self.log_dir / filename
    
    def get_available_log_files(self, days_back: int = 30, use_cache: bool = True) -> List[Tuple[datetime, Path]]:
        """è·å–å¯ç”¨çš„æ—¥å¿—æ–‡ä»¶åˆ—è¡¨ï¼ˆæ”¯æŒç¼“å­˜ï¼Œå‡å°‘æ–‡ä»¶ç³»ç»Ÿæ‰«æï¼‰"""
        current_time = time.time()
        
        # ğŸ”§ ä¼˜åŒ–ï¼šä½¿ç”¨ç¼“å­˜ï¼Œå‡å°‘æ–‡ä»¶ç³»ç»Ÿæ‰«æ
        if use_cache and self.file_cache is not None:
            if current_time - self.file_cache_time < FILE_SCAN_CACHE_TTL:
                logger.debug(f"ä½¿ç”¨æ–‡ä»¶åˆ—è¡¨ç¼“å­˜ï¼ˆ{len(self.file_cache)} ä¸ªæ–‡ä»¶ï¼‰")
                return self.file_cache
        
        files = []
        
        # æ‰«æç›®å½•ä¸­æ‰€æœ‰åŒ¹é…çš„æ—¥å¿—æ–‡ä»¶
        if self.log_dir.exists() and self.log_dir.is_dir():
            # æå–æ–‡ä»¶æ¨¡å¼çš„å‰ç¼€å’Œåç¼€ï¼ˆä¾‹å¦‚ï¼šramnit_{date}.log -> ramnit_*.logï¼‰
            pattern_parts = self.file_pattern.split('{date}')
            if len(pattern_parts) == 2:
                prefix, suffix = pattern_parts
                glob_pattern = f"{prefix}*{suffix}"
                
                # ä½¿ç”¨globæŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶
                for file_path in self.log_dir.glob(glob_pattern):
                    if file_path.is_file():
                        # å°è¯•ä»æ–‡ä»¶åä¸­æå–æ—¥æœŸ
                        try:
                            filename = file_path.name
                            # ç§»é™¤å‰ç¼€å’Œåç¼€ï¼Œæå–æ—¥æœŸéƒ¨åˆ†
                            date_str = filename[len(prefix):-len(suffix)] if suffix else filename[len(prefix):]
                            
                            # å°è¯•è§£ææ—¥æœŸï¼ˆæ”¯æŒ YYYY-MM-DD æ ¼å¼ï¼‰
                            date = datetime.strptime(date_str, '%Y-%m-%d')
                            files.append((date, file_path))
                            logger.debug(f"å‘ç°æ—¥å¿—æ–‡ä»¶: {file_path} (æ—¥æœŸ: {date_str})")
                        except ValueError:
                            logger.warning(f"æ— æ³•ä»æ–‡ä»¶åè§£ææ—¥æœŸ: {file_path}")
                            continue
            else:
                logger.warning(f"æ–‡ä»¶æ¨¡å¼æ ¼å¼ä¸æ­£ç¡®: {self.file_pattern}")
        else:
            logger.warning(f"æ—¥å¿—ç›®å½•ä¸å­˜åœ¨: {self.log_dir}")
        
        # æŒ‰æ—¥æœŸæ’åºï¼ˆä»æ—§åˆ°æ–°ï¼‰
        sorted_files = sorted(files, key=lambda x: x[0])
        
        # æ›´æ–°ç¼“å­˜
        self.file_cache = sorted_files
        self.file_cache_time = current_time
        logger.debug(f"æ›´æ–°æ–‡ä»¶åˆ—è¡¨ç¼“å­˜ï¼ˆ{len(sorted_files)} ä¸ªæ–‡ä»¶ï¼‰")
        
        return sorted_files
    
    def invalidate_cache(self):
        """ä½¿ç¼“å­˜å¤±æ•ˆï¼Œå¼ºåˆ¶é‡æ–°æ‰«æ"""
        self.file_cache = None
        self.file_cache_time = 0
        logger.debug("æ–‡ä»¶åˆ—è¡¨ç¼“å­˜å·²å¤±æ•ˆ")
    
    def load_incomplete_lines(self):
        """åŠ è½½ä¸å®Œæ•´è¡Œç¼“å­˜"""
        try:
            if os.path.exists(INCOMPLETE_LINES_FILE):
                with open(INCOMPLETE_LINES_FILE, 'r') as f:
                    self.incomplete_lines = json.load(f)
                logger.debug(f"åŠ è½½ä¸å®Œæ•´è¡Œç¼“å­˜: {len(self.incomplete_lines)} ä¸ªæ–‡ä»¶")
        except Exception as e:
            logger.warning(f"åŠ è½½ä¸å®Œæ•´è¡Œç¼“å­˜å¤±è´¥: {e}")
            self.incomplete_lines = {}
    
    def save_incomplete_lines(self):
        """ä¿å­˜ä¸å®Œæ•´è¡Œç¼“å­˜"""
        try:
            with open(INCOMPLETE_LINES_FILE, 'w') as f:
                json.dump(self.incomplete_lines, f, indent=2)
            logger.debug(f"ä¿å­˜ä¸å®Œæ•´è¡Œç¼“å­˜: {len(self.incomplete_lines)} ä¸ªæ–‡ä»¶")
        except Exception as e:
            logger.error(f"ä¿å­˜ä¸å®Œæ•´è¡Œç¼“å­˜å¤±è´¥: {e}")
    
    async def read_log_file(self, file_path: Path, processor, start_offset: int = 0, max_lines: int = None) -> tuple[int, int]:
        """å¼‚æ­¥å¢é‡è¯»å–æ—¥å¿—æ–‡ä»¶ï¼ˆæ”¹è¿›ç‰ˆï¼šæ”¯æŒé™åˆ¶è¡Œæ•°ï¼ŒçœŸæ­£åˆ†å—ï¼‰
        
        Args:
            file_path: æ—¥å¿—æ–‡ä»¶è·¯å¾„
            processor: IPå¤„ç†å™¨
            start_offset: å¼€å§‹è¯»å–çš„å­—èŠ‚åç§»é‡
            max_lines: æœ€å¤§è¯»å–è¡Œæ•°ï¼ˆNoneè¡¨ç¤ºè¯»åˆ°æ–‡ä»¶æœ«å°¾ï¼‰
            
        Returns:
            (processed_lines, end_offset): å¤„ç†çš„è¡Œæ•°å’Œç»“æŸåç§»é‡
        """
        if not file_path.exists():
            logger.warning(f"æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return 0, start_offset
        
        file_size = file_path.stat().st_size
        
        # ğŸ”§ ä¿®å¤é—®é¢˜2ï¼šoffset > file_size è¯´æ˜æ–‡ä»¶è¢«æˆªæ–­ï¼Œé‡ç½®ä¸º0
        if start_offset > file_size:
            logger.warning(f"æ–‡ä»¶ {file_path} è¢«æˆªæ–­ (offset={start_offset} > size={file_size})ï¼Œé‡ç½®åç§»é‡ä¸º0")
            start_offset = 0
        elif start_offset == file_size:
            logger.debug(f"æ–‡ä»¶ {file_path} æ— æ–°æ•°æ® (offset={start_offset}, size={file_size})")
            return 0, start_offset
        
        logger.info(f"è¯»å–æ—¥å¿—æ–‡ä»¶: {file_path} (ä»åç§»é‡ {start_offset} å¼€å§‹)")
        processed_lines = 0
        current_offset = start_offset
        file_path_str = str(file_path)
        
        try:
            async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                # å®šä½åˆ°ä¸Šæ¬¡è¯»å–çš„ä½ç½®
                await f.seek(start_offset)
                
                # ğŸ”§ æ”¹è¿›ï¼šæ¢å¤ä¸Šæ¬¡çš„ä¸å®Œæ•´è¡Œ
                buffer = self.incomplete_lines.get(file_path_str, "")
                if buffer:
                    logger.debug(f"æ¢å¤ä¸å®Œæ•´è¡Œ: {buffer[:50]}...")
                
                while True:
                    # ğŸ”§ æ–°å¢ï¼šæ”¯æŒé™åˆ¶è¯»å–è¡Œæ•°
                    if max_lines and processed_lines >= max_lines:
                        logger.debug(f"è¾¾åˆ°æœ€å¤§è¡Œæ•°é™åˆ¶ {max_lines}ï¼Œåœæ­¢è¯»å–")
                        break
                    
                    chunk = await f.read(READ_CHUNK_SIZE)
                    if not chunk:
                        break
                    
                    buffer += chunk
                    lines = buffer.split('\n')
                    
                    # ğŸ”§ æ”¹è¿›ï¼šæ›´å®‰å…¨çš„ä¸å®Œæ•´è¡Œå¤„ç†
                    if len(chunk) < READ_CHUNK_SIZE:  # å·²åˆ°æ–‡ä»¶æœ«å°¾
                        # å¦‚æœæœ€åä¸€è¡Œæ²¡æœ‰æ¢è¡Œç¬¦ï¼Œä¿å­˜åˆ°ç¼“å­˜
                        if not chunk.endswith('\n') and lines[-1]:
                            self.incomplete_lines[file_path_str] = lines[-1]
                            logger.debug(f"ä¿å­˜ä¸å®Œæ•´è¡Œåˆ°ç¼“å­˜: {lines[-1][:50]}...")
                            lines = lines[:-1]
                            buffer = self.incomplete_lines[file_path_str]
                        else:
                            # æœ€åä¸€è¡Œå·²å®Œæ•´ï¼Œæ¸…é™¤ç¼“å­˜
                            if file_path_str in self.incomplete_lines:
                                del self.incomplete_lines[file_path_str]
                            buffer = ""
                    else:
                        buffer = lines[-1]
                        lines = lines[:-1]
                    
                    # å¤„ç†å®Œæ•´çš„è¡Œ
                    line_index = 0
                    for line_index, line in enumerate(lines):
                        if line.strip():
                            await processor.process_line(line.strip())
                            processed_lines += 1
                            
                            # ğŸ”§ æ–°å¢ï¼šè¾¾åˆ°è¡Œæ•°é™åˆ¶å°±åœæ­¢
                            if max_lines and processed_lines >= max_lines:
                                # ğŸ”§ ä¿®å¤é—®é¢˜4ï¼šå°†æœªå¤„ç†çš„è¡Œå›å¡«åˆ°buffer
                                unprocessed_lines = lines[line_index + 1:]
                                if unprocessed_lines:
                                    # æŠŠæœªå¤„ç†çš„å®Œæ•´è¡Œæ‹¼å› buffer
                                    buffer = '\n'.join(unprocessed_lines)
                                    if buffer and not buffer.endswith('\n'):
                                        # bufferæœ€åéƒ¨åˆ†æ˜¯åŸæ¥çš„ä¸å®Œæ•´è¡Œ
                                        pass
                                    logger.debug(f"å›å¡« {len(unprocessed_lines)} è¡Œæœªå¤„ç†æ•°æ®åˆ°buffer")
                                break
                            
                            # æ¯å¤„ç†ä¸€å®šæ•°é‡çš„è¡Œå°±è®©å‡ºæ§åˆ¶æƒ
                            if processed_lines % 1000 == 0:
                                await asyncio.sleep(0.001)
                
                # æ›´æ–°å½“å‰åç§»é‡ï¼ˆå‡å»æœªå¤„ç†çš„bufferé•¿åº¦ï¼‰
                current_offset = await f.tell() - len(buffer.encode('utf-8'))
                
                # ğŸ”§ ä¿®å¤é—®é¢˜3ï¼šå¦‚æœæ–‡ä»¶çœŸæ­£ç»“æŸä¸”æœ‰ä¸å®Œæ•´è¡Œï¼Œå°è¯•å¤„ç†å®ƒ
                if not max_lines:  # åªæœ‰åœ¨è¯»åˆ°æ–‡ä»¶æœ«å°¾æ—¶ï¼ˆè€Œéå› ä¸ºmax_linesé™åˆ¶ï¼‰
                    if file_path_str in self.incomplete_lines:
                        final_line = self.incomplete_lines[file_path_str]
                        # æ£€æŸ¥æ–‡ä»¶å¤§å°æ˜¯å¦è¿˜åœ¨å¢é•¿
                        current_file_size = file_path.stat().st_size
                        if current_offset >= current_file_size:
                            # æ–‡ä»¶ä¸å†å¢é•¿ï¼Œå¤„ç†æœ€åä¸€è¡Œ
                            logger.info(f"æ–‡ä»¶ç»“æŸï¼Œå¤„ç†æœ€åä¸€è¡Œï¼ˆæ— æ¢è¡Œç¬¦ï¼‰: {final_line[:50]}...")
                            if final_line.strip():
                                await processor.process_line(final_line.strip())
                                processed_lines += 1
                                current_offset += len(final_line.encode('utf-8'))
                            # æ¸…é™¤ç¼“å­˜
                            del self.incomplete_lines[file_path_str]
                
                # ä¿å­˜ä¸å®Œæ•´è¡Œç¼“å­˜
                self.save_incomplete_lines()
        
        except Exception as e:
            logger.error(f"è¯»å–æ—¥å¿—æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            # ğŸ”§ æ”¹è¿›ï¼šå‡ºé”™æ—¶ä¹Ÿä¿å­˜è¿›åº¦
            if processed_lines > 0:
                logger.warning(f"å·²å¤„ç† {processed_lines} è¡Œï¼Œä¿å­˜å½“å‰è¿›åº¦")
                return processed_lines, current_offset
            return 0, start_offset
        
        logger.info(f"è¯»å–æ—¥å¿—æ–‡ä»¶å®Œæˆ: {file_path}, å¤„ç†äº† {processed_lines} è¡Œ, æ–°åç§»é‡: {current_offset}")
        return processed_lines, current_offset


class IPProcessor:
    """IPå¤„ç†å™¨ - è´Ÿè´£è§£æã€å»é‡å’Œç¼“å­˜IPæ•°æ®ï¼ˆæ”¹è¿›ç‰ˆï¼šå…¨å±€å»é‡ï¼‰"""
    
    def __init__(self, botnet_type: str, cache_file: str, pending_queue_file: str = PENDING_QUEUE_FILE):
        self.botnet_type = botnet_type
        self.cache_file = cache_file
        self.pending_queue_file = pending_queue_file
        
        # ğŸ”§ æ”¹è¿›ï¼šå…¨å±€å»é‡ï¼Œè€Œä¸æ˜¯ä»…æŒ‰æ—¥æœŸå»é‡
        self.global_ip_cache: Set[str] = set()  # å…¨å±€IPç¼“å­˜ï¼Œç”¨äºå»é‡
        self.ip_last_seen: Dict[str, str] = {}  # è®°å½•IPæœ€åå‡ºç°çš„æ—¥æœŸ
        
        self.daily_ips: Dict[str, Set[str]] = defaultdict(set)  # æŒ‰æ—¥æœŸåˆ†ç»„çš„IP
        self.daily_ips_with_time: Dict[str, List[Dict]] = defaultdict(list)  # åŒ…å«æ—¶é—´æˆ³çš„IPæ•°æ®
        self.processed_count = 0
        self.duplicate_count = 0
        self.global_duplicate_count = 0  # å…¨å±€é‡å¤è®¡æ•°
        
        self.load_cache()
        self.load_pending_queue()  # åŠ è½½æœªä¸Šä¼ çš„æ•°æ®
        self.last_persist_time = time.time()
    
    def load_cache(self):
        """åŠ è½½IPç¼“å­˜ï¼ˆç”¨äºå…¨å±€å»é‡ï¼‰"""
        try:
            if os.path.exists(self.cache_file):
                with open(self.cache_file, 'r') as f:
                    cache_data = json.load(f)
                
                # ğŸ”§ æ”¹è¿›ï¼šåŠ è½½ç”¨äºå…¨å±€å»é‡
                self.global_ip_cache = set()
                self.ip_last_seen = {}
                
                for ip_data in cache_data.get('ips', []):
                    ip = ip_data['ip']
                    self.global_ip_cache.add(ip)
                    # è®°å½•æœ€åå‡ºç°æ—¥æœŸ
                    if 'date' in ip_data:
                        self.ip_last_seen[ip] = ip_data['date']
                
                logger.info(f"åŠ è½½IPç¼“å­˜: {len(self.global_ip_cache)} ä¸ªIPï¼ˆç”¨äºå…¨å±€å»é‡ï¼‰")
        except Exception as e:
            logger.warning(f"åŠ è½½IPç¼“å­˜å¤±è´¥: {e}")
            self.global_ip_cache = set()
            self.ip_last_seen = {}
    
    def save_cache(self):
        """ä¿å­˜IPç¼“å­˜ï¼ˆåŒ…å«æœ€åå‡ºç°æ—¥æœŸï¼‰"""
        try:
            cache_data = {
                'updated_at': datetime.now().isoformat(),
                'ips': [
                    {
                        'ip': ip,
                        'date': self.ip_last_seen.get(ip, datetime.now().strftime('%Y-%m-%d')),
                        'timestamp': datetime.now().isoformat()
                    }
                    for ip in self.global_ip_cache
                ]
            }
            
            with open(self.cache_file, 'w') as f:
                json.dump(cache_data, f, indent=2)
                
            logger.debug(f"ä¿å­˜IPç¼“å­˜: {len(self.global_ip_cache)} ä¸ªIP")
        except Exception as e:
            logger.error(f"ä¿å­˜IPç¼“å­˜å¤±è´¥: {e}")
    
    def load_pending_queue(self):
        """åŠ è½½å¾…ä¸Šä¼ é˜Ÿåˆ—ï¼ˆæ¢å¤æœªä¸Šä¼ çš„æ•°æ®ï¼‰"""
        try:
            if os.path.exists(self.pending_queue_file):
                with open(self.pending_queue_file, 'r') as f:
                    queue_data = json.load(f)
                    
                # æ¢å¤åˆ°å†…å­˜ä¸­
                for item in queue_data.get('items', []):
                    date = item['date']
                    if date not in self.daily_ips_with_time:
                        self.daily_ips_with_time[date] = []
                    self.daily_ips_with_time[date].append(item)
                    self.daily_ips[date].add(item['ip'])
                    
                logger.info(f"ä»é˜Ÿåˆ—æ¢å¤ {len(queue_data.get('items', []))} æ¡å¾…ä¸Šä¼ æ•°æ®")
        except Exception as e:
            logger.warning(f"åŠ è½½å¾…ä¸Šä¼ é˜Ÿåˆ—å¤±è´¥: {e}")
    
    def save_pending_queue(self):
        """æŒä¹…åŒ–å¾…ä¸Šä¼ é˜Ÿåˆ—åˆ°ç£ç›˜"""
        try:
            all_items = []
            for date, ip_data_list in self.daily_ips_with_time.items():
                all_items.extend(ip_data_list)
            
            queue_data = {
                'updated_at': datetime.now().isoformat(),
                'count': len(all_items),
                'items': all_items
            }
            
            with open(self.pending_queue_file, 'w') as f:
                json.dump(queue_data, f, indent=2)
                
            logger.debug(f"æŒä¹…åŒ–å¾…ä¸Šä¼ é˜Ÿåˆ—: {len(all_items)} æ¡")
            self.last_persist_time = time.time()
        except Exception as e:
            logger.error(f"æŒä¹…åŒ–é˜Ÿåˆ—å¤±è´¥: {e}")
    
    def should_persist(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦æŒä¹…åŒ–ï¼ˆå®šæœŸæˆ–æ•°æ®é‡å¤§æ—¶ï¼‰"""
        total_items = sum(len(ips) for ips in self.daily_ips_with_time.values())
        time_elapsed = time.time() - self.last_persist_time
        
        # æ¡ä»¶ï¼šæ•°æ®é‡è¶…è¿‡é˜ˆå€¼ æˆ– è·ç¦»ä¸Šæ¬¡æŒä¹…åŒ–è¶…è¿‡60ç§’
        return total_items > 1000 or time_elapsed > 60
    
    async def process_line(self, line: str):
        """å¤„ç†å•è¡Œæ—¥å¿—ï¼Œæå–IPåœ°å€å’Œæ—¶é—´æˆ³ï¼ˆæ”¹è¿›ç‰ˆï¼šå…¨å±€å»é‡ï¼‰"""
        self.processed_count += 1
        
        # è§£ææ—¥å¿—æ ¼å¼: 2025-11-12 10:32:32,125.162.162.237
        ip_data = self.extract_ip_and_timestamp_from_line(line)
        
        if ip_data and self.is_valid_ip(ip_data['ip']):
            ip = ip_data['ip']
            log_date = ip_data['date']
            
            # ğŸ”§ æ”¹è¿›ï¼šå…¨å±€å»é‡ç­–ç•¥
            # å¦‚æœIPå·²ç»åœ¨å…¨å±€ç¼“å­˜ä¸­ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
            if ip in self.global_ip_cache:
                last_date = self.ip_last_seen.get(ip)
                
                # å¦‚æœæ˜¯åŒä¸€å¤©å‡ºç°ï¼Œè·³è¿‡
                if last_date == log_date:
                    self.duplicate_count += 1
                    return
                
                # å¦‚æœæ˜¯ä¸åŒå¤©å‡ºç°ï¼Œå¯èƒ½éœ€è¦æ›´æ–°updated_at
                # è¿™é‡Œå¯ä»¥é€‰æ‹©ï¼šA) æ¯æ¬¡éƒ½ä¸Šä¼ æ›´æ–° B) è¶…è¿‡ä¸€å®šå¤©æ•°æ‰æ›´æ–°
                # å½“å‰ç­–ç•¥ï¼šè·¨æ—¥æœŸçš„IPä¹Ÿç®—é‡å¤ï¼Œå‡å°‘é‡å¤ä¸Šä¼ 
                self.global_duplicate_count += 1
                
                # æ›´æ–°æœ€åå‡ºç°æ—¥æœŸ
                self.ip_last_seen[ip] = log_date
                return
            
            # æ–°IPï¼Œæ·»åŠ åˆ°å¾…ä¸Šä¼ é˜Ÿåˆ—
            if log_date not in self.daily_ips_with_time:
                self.daily_ips_with_time[log_date] = []
            
            self.daily_ips_with_time[log_date].append(ip_data)
            self.daily_ips[log_date].add(ip)
            
            # æ›´æ–°å…¨å±€ç¼“å­˜
            self.global_ip_cache.add(ip)
            self.ip_last_seen[ip] = log_date
    
    def extract_ip_and_timestamp_from_line(self, line: str) -> Optional[Dict]:
        """ä»æ—¥å¿—è¡Œä¸­æå–IPåœ°å€å’Œæ—¶é—´æˆ³"""
        try:
            line = line.strip()
            if not line:
                return None
            
            # å°è¯•ä»è¡Œé¦–æå–æ—¶é—´æˆ³
            # æ”¯æŒæ ¼å¼ï¼š
            # 1. 2025/07/03 09:31:24 æ–°IPé¦–æ¬¡è¿æ¥: 180.254.163.108
            # 2. 2025-11-12 10:32:32,125.162.162.237
            # 3. 2025-11-12 10:32:32 å…¶ä»–æ–‡æœ¬ 125.162.162.237
            
            timestamp_str = None
            log_time = None
            
            # å°è¯•è§£æè¡Œé¦–çš„æ—¶é—´æˆ³ï¼ˆä¸¤ç§æ ¼å¼ï¼‰
            time_formats = [
                '%Y/%m/%d %H:%M:%S',  # 2025/07/03 09:31:24
                '%Y-%m-%d %H:%M:%S',  # 2025-11-12 10:32:32
            ]
            
            # æå–è¡Œé¦–çš„æ—¶é—´æˆ³å­—ç¬¦ä¸²ï¼ˆå‰19ä¸ªå­—ç¬¦ï¼‰
            if len(line) >= 19:
                potential_timestamp = line[:19]
                for fmt in time_formats:
                    try:
                        log_time = datetime.strptime(potential_timestamp, fmt)
                        timestamp_str = potential_timestamp
                        logger.debug(f"æˆåŠŸè§£ææ—¶é—´æˆ³: {timestamp_str} -> {log_time}")
                        break
                    except ValueError:
                        continue
            
            # å¦‚æœæ²¡æœ‰è§£æåˆ°æ—¶é—´æˆ³ï¼Œä½¿ç”¨å½“å‰æ—¶é—´
            if not log_time:
                logger.warning(f"æœªèƒ½ä»æ—¥å¿—è¡Œæå–æ—¶é—´æˆ³ï¼Œä½¿ç”¨å½“å‰æ—¶é—´: {line[:50]}...")
                log_time = datetime.now()
            
            # æå–IPåœ°å€
            ips = IP_REGEX.findall(line)
            if ips:
                # è¿‡æ»¤æ‰æ—¶é—´æˆ³ä¸­å¯èƒ½è¢«è¯¯è¯†åˆ«çš„æ•°å­—
                valid_ips = [ip for ip in ips if self.is_valid_ip(ip)]
                if valid_ips:
                    return {
                        'ip': valid_ips[0],
                        'timestamp': log_time.isoformat(),
                        'date': log_time.strftime('%Y-%m-%d'),
                        'botnet_type': self.botnet_type
                    }
            
            return None
        except Exception as e:
            logger.debug(f"æå–IPå’Œæ—¶é—´æˆ³å¤±è´¥: {line[:50]}... é”™è¯¯: {e}")
            return None
    
    def extract_ip_from_line(self, line: str) -> Optional[str]:
        """ä»æ—¥å¿—è¡Œä¸­æå–IPåœ°å€ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰"""
        ip_data = self.extract_ip_and_timestamp_from_line(line)
        return ip_data['ip'] if ip_data else None
    
    def is_valid_ip(self, ip: str) -> bool:
        """éªŒè¯IPåœ°å€æ˜¯å¦æœ‰æ•ˆ"""
        try:
            parts = ip.split('.')
            if len(parts) != 4:
                return False
            
            for part in parts:
                num = int(part)
                if num < 0 or num > 255:
                    return False
            
            # è¿‡æ»¤ç§æœ‰IPå’Œç‰¹æ®ŠIP
            if ip.startswith(('127.', '10.', '192.168.', '169.254.')):
                return False
            if ip.startswith('172.'):
                second_octet = int(parts[1])
                if 16 <= second_octet <= 31:
                    return False
            
            return True
        except:
            return False
    
    def get_new_ips_for_upload(self, max_count: int = None) -> List[Dict]:
        """è·å–éœ€è¦ä¸Šä¼ çš„æ–°IPæ•°æ®ï¼ˆæŒ‰æ—¶é—´æˆ³æ’åºï¼‰"""
        all_new_ips = []
        
        # ä¼˜å…ˆä½¿ç”¨åŒ…å«æ—¶é—´æˆ³çš„æ•°æ®ï¼Œå¹¶æŒ‰æ—¥æœŸæ’åº
        sorted_dates = sorted(self.daily_ips_with_time.keys())
        for date in sorted_dates:
            all_new_ips.extend(self.daily_ips_with_time[date])
        
        # å¦‚æœæ²¡æœ‰æ—¶é—´æˆ³æ•°æ®ï¼Œå›é€€åˆ°æ—§æ ¼å¼ï¼ˆå…¼å®¹æ€§ï¼‰
        if not all_new_ips:
            sorted_dates = sorted(self.daily_ips.keys())
            for date in sorted_dates:
                for ip in self.daily_ips[date]:
                    all_new_ips.append({
                        'ip': ip,
                        'date': date,
                        'botnet_type': self.botnet_type,
                        'timestamp': datetime.now().isoformat()
                    })
        
        # é™åˆ¶æ•°é‡
        if max_count and len(all_new_ips) > max_count:
            return all_new_ips[:max_count]
        
        return all_new_ips
    
    def clear_uploaded_ips(self, uploaded_count: int):
        """æ¸…ç†å·²ä¸Šä¼ çš„IPï¼ˆåŒ…å«æ—¶é—´æˆ³æ•°æ®ï¼‰"""
        # ä¼˜å…ˆæ¸…ç†æ—¶é—´æˆ³æ•°æ®
        cleared_count = 0
        
        for date in list(self.daily_ips_with_time.keys()):
            ip_data_list = self.daily_ips_with_time[date]
            
            # æ¸…ç†æŒ‡å®šæ•°é‡çš„IPæ•°æ®
            while ip_data_list and cleared_count < uploaded_count:
                ip_data = ip_data_list.pop(0)
                ip = ip_data['ip']
                
                # åŒæ—¶ä»daily_ipsä¸­ç§»é™¤
                if date in self.daily_ips:
                    self.daily_ips[date].discard(ip)
                
                cleared_count += 1
            
            # å¦‚æœè¯¥æ—¥æœŸçš„IPæ•°æ®ä¸ºç©ºï¼Œåˆ é™¤è¯¥æ—¥æœŸ
            if not ip_data_list:
                del self.daily_ips_with_time[date]
            
            # å¦‚æœè¯¥æ—¥æœŸçš„IPé›†åˆä¸ºç©ºï¼Œåˆ é™¤è¯¥æ—¥æœŸ
            if date in self.daily_ips and not self.daily_ips[date]:
                del self.daily_ips[date]
            
            if cleared_count >= uploaded_count:
                break
        
        # å¦‚æœæ—¶é—´æˆ³æ•°æ®ä¸è¶³ï¼Œç»§ç»­æ¸…ç†æ™®é€šIPæ•°æ®ï¼ˆå…¼å®¹æ€§ï¼‰
        if cleared_count < uploaded_count:
            for date in list(self.daily_ips.keys()):
                ips_list = list(self.daily_ips[date])
                
                for ip in ips_list:
                    if cleared_count >= uploaded_count:
                        break
                        
                    self.daily_ips[date].discard(ip)
                    cleared_count += 1
                
                # å¦‚æœè¯¥æ—¥æœŸçš„IPé›†åˆä¸ºç©ºï¼Œåˆ é™¤è¯¥æ—¥æœŸ
                if not self.daily_ips[date]:
                    del self.daily_ips[date]
                
                if cleared_count >= uploaded_count:
                    break
        
        logger.info(f"æ¸…ç†å·²ä¸Šä¼ çš„IP: {cleared_count} ä¸ª")
    
    def get_stats(self) -> Dict:
        """è·å–å¤„ç†ç»Ÿè®¡ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        # ä¿®å¤ï¼šç»Ÿè®¡daily_ips_with_timeä¸­çš„æ•°æ®
        total_new_ips = sum(len(ip_list) for ip_list in self.daily_ips_with_time.values())
        return {
            'processed_lines': self.processed_count,
            'duplicate_count': self.duplicate_count,
            'global_duplicate_count': self.global_duplicate_count,  # æ–°å¢
            'cached_ips': len(self.global_ip_cache),
            'new_ips_pending': total_new_ips,
            'unique_ips_total': len(self.global_ip_cache) + total_new_ips
        }
    
    def check_memory_pressure(self) -> bool:
        """æ£€æŸ¥å†…å­˜å‹åŠ›"""
        total_items = sum(len(ip_list) for ip_list in self.daily_ips_with_time.values())
        return total_items >= MAX_MEMORY_IPS


class RemoteUploader:
    """è¿œç«¯ä¸Šä¼ å™¨ - è´Ÿè´£å°†å¤„ç†åçš„æ•°æ®ä¸Šä¼ åˆ°æœ¬åœ°æœåŠ¡å™¨"""
    
    def __init__(self):
        self.upload_count = 0
        self.error_count = 0
        self.session = None
    
    async def create_session(self):
        """åˆ›å»ºHTTPä¼šè¯"""
        if self.session is None:
            # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°120ç§’ï¼Œé€‚åº”é¦–æ¬¡è¡¨ç»“æ„å‡çº§ç­‰è€—æ—¶æ“ä½œ
            # æ³¨æ„ï¼šæ­£å¸¸ä¸Šä¼ åº”è¯¥åœ¨å‡ ç§’å†…å®Œæˆï¼Œä½†é¦–æ¬¡å¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´
            timeout = aiohttp.ClientTimeout(total=120, connect=10)
            self.session = aiohttp.ClientSession(timeout=timeout)
    
    async def close_session(self):
        """å…³é—­HTTPä¼šè¯"""
        if self.session:
            await self.session.close()
            self.session = None
    
    async def check_server_online(self) -> bool:
        """
        æ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦åœ¨çº¿
        è¿”å›: Trueè¡¨ç¤ºåœ¨çº¿ï¼ŒFalseè¡¨ç¤ºç¦»çº¿
        """
        await self.create_session()
        
        try:
            # å°è¯•è¿æ¥åˆ°æœåŠ¡å™¨çš„å¥åº·æ£€æŸ¥ç«¯ç‚¹
            # å¦‚æœæœåŠ¡å™¨æ²¡æœ‰ä¸“é—¨çš„å¥åº·æ£€æŸ¥ç«¯ç‚¹ï¼Œä½¿ç”¨æ™®é€šçš„GETè¯·æ±‚
            headers = {
                "X-API-Key": API_KEY
            }
            
            # ä½¿ç”¨è¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´è¿›è¡Œå¥åº·æ£€æŸ¥
            timeout = aiohttp.ClientTimeout(total=SERVER_CHECK_TIMEOUT)
            
            async with self.session.get(
                API_ENDPOINT,
                headers=headers,
                timeout=timeout
            ) as response:
                # åªè¦èƒ½æ”¶åˆ°å“åº”ï¼ˆæ— è®ºçŠ¶æ€ç ï¼‰ï¼Œå°±è®¤ä¸ºæœåŠ¡å™¨åœ¨çº¿
                # å› ä¸ºæœåŠ¡å™¨å¯èƒ½è¿”å›404æˆ–å…¶ä»–é”™è¯¯ï¼Œä½†è¿™è¡¨æ˜æœåŠ¡å™¨æ˜¯è¿è¡Œçš„
                logger.info(f" æœåŠ¡å™¨åœ¨çº¿æ£€æŸ¥é€šè¿‡ (HTTP {response.status})")
                return True
                
        except asyncio.TimeoutError:
            logger.warning(" æœåŠ¡å™¨è¿æ¥è¶…æ—¶ï¼Œå¯èƒ½ç¦»çº¿")
            return False
            
        except aiohttp.ClientConnectorError as e:
            logger.warning(f" æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨: {e}")
            return False
            
        except aiohttp.ClientError as e:
            logger.warning(f" æœåŠ¡å™¨è¿æ¥é”™è¯¯: {e}")
            return False
            
        except Exception as e:
            logger.error(f" æœåŠ¡å™¨åœ¨çº¿æ£€æŸ¥å¼‚å¸¸: {e}")
            return False
    
    async def upload_ips(self, ip_data: List[Dict]) -> bool:
        """å¼‚æ­¥ä¸Šä¼ IPæ•°æ®åˆ°æœ¬åœ°æœåŠ¡å™¨"""
        if not ip_data:
            return True
        
        await self.create_session()
        
        headers = {
            "Content-Type": "application/json",
            "X-API-Key": API_KEY
        }
        
        data = {
            "botnet_type": BOTNET_TYPE,
            "ip_data": ip_data,
            "source_ip": await self.get_local_ip()
        }
        
        for attempt in range(MAX_RETRIES):
            try:
                logger.info(f"ä¸Šä¼  {len(ip_data)} ä¸ªIP (å°è¯• {attempt + 1}/{MAX_RETRIES})")
                
                async with self.session.post(
                    API_ENDPOINT,
                    json=data,
                    headers=headers
                ) as response:
                    
                    if response.status == 200:
                        result = await response.json()
                        logger.info(f" ä¸Šä¼ æˆåŠŸ: {result.get('received_count', len(ip_data))} ä¸ªIP")
                        self.upload_count += result.get('received_count', len(ip_data))
                        return True
                        
                    elif response.status == 401:
                        logger.error(" è®¤è¯å¤±è´¥: APIå¯†é’¥æ— æ•ˆ")
                        return False
                        
                    elif response.status == 403:
                        logger.error(" æƒé™ä¸è¶³: IPæœªåœ¨ç™½åå•ä¸­")
                        return False
                        
                    else:
                        error_text = await response.text()
                        logger.warning(f" ä¸Šä¼ å¤±è´¥ (HTTP {response.status}): {error_text}")
                        
            except aiohttp.ClientError as e:
                logger.warning(f" ç½‘ç»œé”™è¯¯: {e}")
                
            except Exception as e:
                logger.error(f" ä¸Šä¼ å¼‚å¸¸: {e}")
            
            # é‡è¯•å»¶è¿Ÿ
            if attempt < MAX_RETRIES - 1:
                logger.info(f"ç­‰å¾… {RETRY_DELAY} ç§’åé‡è¯•...")
                await asyncio.sleep(RETRY_DELAY)
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        logger.error(f" ä¸Šä¼ å¤±è´¥: å·²é‡è¯• {MAX_RETRIES} æ¬¡")
        self.error_count += 1
        return False
    
    async def get_local_ip(self) -> str:
        """è·å–æœ¬æœºIP"""
        try:
            # ä½¿ç”¨å¼‚æ­¥æ–¹å¼è·å–IP
            import socket
            loop = asyncio.get_event_loop()
            
            def _get_ip():
                s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
                s.connect(("8.8.8.8", 80))
                ip = s.getsockname()[0]
                s.close()
                return ip
            
            return await loop.run_in_executor(None, _get_ip)
        except:
            return "unknown"


class AsyncLogProcessor:
    """å¼‚æ­¥æ—¥å¿—å¤„ç†å™¨ - åè°ƒLogReaderã€IPProcessorå’ŒRemoteUploaderï¼ˆæ”¹è¿›ç‰ˆï¼‰"""
    
    def __init__(self):
        self.log_reader = LogReader(LOG_DIR, LOG_FILE_PATTERN)
        self.ip_processor = IPProcessor(BOTNET_TYPE, DUPLICATE_CACHE_FILE, PENDING_QUEUE_FILE)
        self.uploader = RemoteUploader()
        self.state = self.load_state()
        self.server_offline_count = 0  # æœåŠ¡å™¨è¿ç»­ç¦»çº¿æ¬¡æ•°è®¡æ•°å™¨
        self.upload_in_progress = False  # ä¸Šä¼ è¿›è¡Œä¸­æ ‡å¿—
    
    def load_state(self) -> Dict:
        """åŠ è½½å¤„ç†çŠ¶æ€"""
        try:
            if os.path.exists(STATE_FILE):
                with open(STATE_FILE, 'r') as f:
                    state = json.load(f)
                    logger.info(f"åŠ è½½çŠ¶æ€: æ€»å…±å¤„ç† {state.get('total_processed', 0)} è¡Œ")
                    return state
        except Exception as e:
            logger.warning(f"åŠ è½½çŠ¶æ€å¤±è´¥: {e}")
        
        return {
            'file_offsets': {},  # è®°å½•æ¯ä¸ªæ–‡ä»¶çš„è¯»å–åç§»é‡ {filepath: offset}
            'last_upload_time': None,
            'total_processed': 0
        }
    
    def load_file_offsets(self) -> Dict[str, int]:
        """åŠ è½½æ–‡ä»¶è¯»å–åç§»é‡"""
        try:
            if os.path.exists(OFFSET_STATE_FILE):
                with open(OFFSET_STATE_FILE, 'r') as f:
                    offsets = json.load(f)
                    logger.info(f"åŠ è½½æ–‡ä»¶åç§»é‡: {len(offsets)} ä¸ªæ–‡ä»¶")
                    return offsets
        except Exception as e:
            logger.warning(f"åŠ è½½æ–‡ä»¶åç§»é‡å¤±è´¥: {e}")
        return {}
    
    def save_file_offsets(self, offsets: Dict[str, int]):
        """ä¿å­˜æ–‡ä»¶è¯»å–åç§»é‡"""
        try:
            with open(OFFSET_STATE_FILE, 'w') as f:
                json.dump(offsets, f, indent=2)
            logger.debug(f"ä¿å­˜æ–‡ä»¶åç§»é‡: {len(offsets)} ä¸ªæ–‡ä»¶")
        except Exception as e:
            logger.error(f"ä¿å­˜æ–‡ä»¶åç§»é‡å¤±è´¥: {e}")
    
    def save_state(self):
        """ä¿å­˜å¤„ç†çŠ¶æ€"""
        try:
            with open(STATE_FILE, 'w') as f:
                json.dump(self.state, f, indent=2)
            logger.debug("ä¿å­˜çŠ¶æ€æˆåŠŸ")
        except Exception as e:
            logger.error(f"ä¿å­˜çŠ¶æ€å¤±è´¥: {e}")
    
    async def process_log_files_and_upload(self):
        """å¤„ç†æ—¥å¿—æ–‡ä»¶å¹¶å®ç°çœŸæ­£çš„æµå¼ä¸Šä¼ """
        # è·å–å¯ç”¨çš„æ—¥å¿—æ–‡ä»¶
        log_files = self.log_reader.get_available_log_files()
        
        if not log_files:
            logger.info("æ²¡æœ‰æ‰¾åˆ°æ—¥å¿—æ–‡ä»¶")
            return
        
        logger.info(f"å‘ç° {len(log_files)} ä¸ªæ—¥å¿—æ–‡ä»¶")
        
        # åŠ è½½æ–‡ä»¶åç§»é‡
        file_offsets = self.load_file_offsets()
        
        # å¤„ç†æ¯ä¸ªæ–‡ä»¶ï¼ˆåŒ…æ‹¬å·²å¤„ç†è¿‡çš„æ–‡ä»¶ï¼Œä»¥è¯»å–å¢é‡æ•°æ®ï¼‰
        for date, file_path in log_files:
            file_path_str = str(file_path)
            start_offset = file_offsets.get(file_path_str, 0)
            
            logger.info(f"å¤„ç†æ—¥å¿—æ–‡ä»¶: {file_path} (æ—¥æœŸ: {date.strftime('%Y-%m-%d')})")
            
            try:
                # ğŸ”§ æ”¹è¿›ï¼šæµå¼å¤„ç† - åˆ†å—è¯»å–å’Œä¸Šä¼ 
                await self.process_file_streaming(
                    file_path, file_path_str, start_offset, file_offsets
                )
                    
            except Exception as e:
                logger.error(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}", exc_info=True)
                # ä¿å­˜å½“å‰è¿›åº¦
                self.ip_processor.save_pending_queue()
                # ğŸ”§ æ”¹è¿›ï¼šä¸è¦å› ä¸ºä¸€ä¸ªæ–‡ä»¶å¤±è´¥å°±é€€å‡ºï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª
                logger.warning(f"è·³è¿‡å¤±è´¥æ–‡ä»¶ {file_path}ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæ–‡ä»¶")
                continue
    
    async def process_file_streaming(self, file_path: Path, file_path_str: str, 
                                     start_offset: int, file_offsets: Dict[str, int]):
        """æµå¼å¤„ç†å•ä¸ªæ–‡ä»¶ - è¾¹è¯»è¾¹ä¼ ï¼ˆçœŸæ­£çš„åˆ†å—å¤„ç†ï¼‰"""
        chunk_lines = 5000  # ğŸ”§ æ”¹è¿›ï¼šæ¯æ¬¡åªè¯»å–5000è¡Œï¼ŒçœŸæ­£åˆ†å—
        total_processed = 0
        current_offset = start_offset
        last_saved_offset = start_offset
        
        logger.info(f"å¼€å§‹æµå¼å¤„ç†æ–‡ä»¶: {file_path}")
        
        while True:
            # æ£€æŸ¥å†…å­˜å‹åŠ›
            if self.ip_processor.check_memory_pressure():
                logger.warning(f"å†…å­˜å‹åŠ›è¿‡å¤§ï¼Œæš‚åœè¯»å–å¹¶å¼ºåˆ¶ä¸Šä¼ ")
                await self.force_upload_all()
            
            # ğŸ”§ å…³é”®æ”¹è¿›ï¼šæ¯æ¬¡åªè¯»å–æŒ‡å®šè¡Œæ•°ï¼ŒçœŸæ­£åˆ†å—
            batch_processed, new_offset = await self.log_reader.read_log_file(
                file_path, self.ip_processor, current_offset, max_lines=chunk_lines
            )
            
            if batch_processed == 0:
                # æ²¡æœ‰æ–°æ•°æ®äº†
                logger.info(f"æ–‡ä»¶ {file_path.name} æ— æ›´å¤šæ•°æ®")
                break
            
            total_processed += batch_processed
            current_offset = new_offset
            
            logger.info(f"æ‰¹æ¬¡å¤„ç†: {batch_processed} è¡Œ, ç´¯è®¡: {total_processed} è¡Œ, å½“å‰åç§»é‡: {current_offset}")
            
            # ğŸ”§ å…³é”®æ”¹è¿›ï¼šé¢‘ç¹ä¸Šä¼ ï¼Œé¿å…ç§¯å‹
            await self.upload_if_needed_aggressive()
            
            # å®šæœŸæŒä¹…åŒ–å¾…ä¸Šä¼ é˜Ÿåˆ—
            if self.ip_processor.should_persist():
                self.ip_processor.save_pending_queue()
            
            # ğŸ”§ å…³é”®æ”¹è¿›ï¼šåªåœ¨ä¸Šä¼ æˆåŠŸåæ‰ä¿å­˜åç§»é‡
            stats = self.ip_processor.get_stats()
            if stats['new_ips_pending'] < MIN_UPLOAD_BATCH:
                # å¾…ä¸Šä¼ æ•°æ®å°‘ï¼Œè¯´æ˜åˆšä¸Šä¼ è¿‡ï¼Œå¯ä»¥å®‰å…¨ä¿å­˜åç§»é‡
                file_offsets[file_path_str] = current_offset
                self.save_file_offsets(file_offsets)
                self.state['total_processed'] += (current_offset - last_saved_offset)
                self.save_state()
                last_saved_offset = current_offset
                logger.debug(f"å®‰å…¨ä¿å­˜åç§»é‡: {current_offset}")
        
        # ğŸ”§ æ–°å¢ï¼šæ–‡ä»¶å¤„ç†å®Œæˆåï¼Œä¸Šä¼ å‰©ä½™æ•°æ®
        if total_processed > 0:
            logger.info(f"æ–‡ä»¶å¤„ç†å®Œæˆ: {file_path}, æ€»å…±å¤„ç† {total_processed} è¡Œ")
            
            # ä¸Šä¼ å‰©ä½™çš„æ•°æ®ï¼ˆå³ä½¿æ²¡è¾¾åˆ°é˜ˆå€¼ï¼‰
            stats = self.ip_processor.get_stats()
            if stats['new_ips_pending'] > 0:
                logger.info(f"ä¸Šä¼ æ–‡ä»¶å‰©ä½™çš„ {stats['new_ips_pending']} æ¡æ•°æ®")
                await self.upload_new_ips()
                
                # ä¸Šä¼ æˆåŠŸåä¿å­˜æœ€ç»ˆåç§»é‡
                stats_after = self.ip_processor.get_stats()
                if stats_after['new_ips_pending'] == 0:
                    file_offsets[file_path_str] = current_offset
                    self.save_file_offsets(file_offsets)
                    logger.info(f"âœ“ æ–‡ä»¶ {file_path.name} å®Œå…¨å¤„ç†å¹¶ä¸Šä¼ æˆåŠŸ")
    
    async def upload_if_needed(self):
        """æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„IPéœ€è¦ä¸Šä¼ ï¼Œå¦‚æœæœ‰åˆ™ç«‹å³ä¸Šä¼ """
        ip_stats = self.ip_processor.get_stats()
        pending_ips = ip_stats['new_ips_pending']
        
        # å¦‚æœå¾…ä¸Šä¼ IPè¾¾åˆ°æ‰¹æ¬¡å¤§å°ï¼Œç«‹å³ä¸Šä¼ 
        if pending_ips >= BATCH_SIZE:
            logger.info(f"å¾…ä¸Šä¼ IPå·²è¾¾åˆ°æ‰¹æ¬¡å¤§å°({pending_ips} >= {BATCH_SIZE})ï¼Œå¼€å§‹ä¸Šä¼ ")
            await self.upload_new_ips()
    
    async def upload_if_needed_aggressive(self):
        """æ›´æ¿€è¿›çš„ä¸Šä¼ ç­–ç•¥ - ç”¨äºæµå¼å¤„ç†"""
        if self.upload_in_progress:
            return
        
        ip_stats = self.ip_processor.get_stats()
        pending_ips = ip_stats['new_ips_pending']
        
        # ğŸ”§ æ”¹è¿›ï¼šé™ä½ä¸Šä¼ é˜ˆå€¼ï¼Œæ›´é¢‘ç¹ä¸Šä¼ 
        if pending_ips >= FORCE_UPLOAD_THRESHOLD:
            logger.info(f"è¾¾åˆ°å¼ºåˆ¶ä¸Šä¼ é˜ˆå€¼({pending_ips} >= {FORCE_UPLOAD_THRESHOLD})")
            await self.upload_new_ips()
        elif pending_ips >= MIN_UPLOAD_BATCH:
            # å³ä½¿æ²¡è¾¾åˆ°å¤§æ‰¹æ¬¡ï¼Œä¹Ÿå°è¯•ä¸Šä¼ å°æ‰¹æ¬¡
            logger.info(f"å°æ‰¹æ¬¡ä¸Šä¼ ({pending_ips} >= {MIN_UPLOAD_BATCH})")
            await self.upload_new_ips()
    
    async def force_upload_all(self):
        """å¼ºåˆ¶ä¸Šä¼ æ‰€æœ‰å¾…ä¸Šä¼ æ•°æ®"""
        ip_stats = self.ip_processor.get_stats()
        pending_ips = ip_stats['new_ips_pending']
        
        if pending_ips == 0:
            return
        
        logger.warning(f"å¼ºåˆ¶ä¸Šä¼ æ‰€æœ‰æ•°æ®: {pending_ips} æ¡")
        
        while pending_ips > 0:
            await self.upload_new_ips()
            ip_stats = self.ip_processor.get_stats()
            new_pending = ip_stats['new_ips_pending']
            
            if new_pending >= pending_ips:
                # æ²¡æœ‰å‡å°‘ï¼Œå¯èƒ½ä¸Šä¼ å¤±è´¥
                logger.error("ä¸Šä¼ å¤±è´¥ï¼Œæ— æ³•å‡å°‘å¾…ä¸Šä¼ é˜Ÿåˆ—")
                break
            
            pending_ips = new_pending
    
    async def upload_new_ips(self):
        """ä¸Šä¼ æ–°å‘ç°çš„IPï¼ˆæ”¹è¿›ç‰ˆï¼šæŒä¹…åŒ–ä¿æŠ¤ï¼‰"""
        if self.upload_in_progress:
            logger.debug("ä¸Šä¼ æ­£åœ¨è¿›è¡Œä¸­ï¼Œè·³è¿‡")
            return
        
        self.upload_in_progress = True
        
        try:
            new_ips = self.ip_processor.get_new_ips_for_upload(BATCH_SIZE)
            
            if not new_ips:
                logger.debug("æ²¡æœ‰æ–°IPéœ€è¦ä¸Šä¼ ")
                return
            
            logger.info(f"å‡†å¤‡ä¸Šä¼  {len(new_ips)} ä¸ªæ–°IP")
            
            # ğŸ”§ å…³é”®æ”¹è¿›ï¼šä¸Šä¼ å‰å…ˆæŒä¹…åŒ–åˆ°ç£ç›˜
            self.ip_processor.save_pending_queue()
            
            # ä¸Šä¼ IPæ•°æ®
            success = await self.uploader.upload_ips(new_ips)
            
            if success:
                # ğŸ”§ åªåœ¨ä¸Šä¼ æˆåŠŸåæ‰æ¸…ç†
                self.ip_processor.clear_uploaded_ips(len(new_ips))
                self.state['last_upload_time'] = datetime.now().isoformat()
                
                # ä¿å­˜IPç¼“å­˜å’ŒçŠ¶æ€
                self.ip_processor.save_cache()
                self.ip_processor.save_pending_queue()  # æ›´æ–°æŒä¹…åŒ–é˜Ÿåˆ—
                self.save_state()
                
                logger.info(f"âœ“ ä¸Šä¼ æˆåŠŸï¼Œç´¯è®¡ä¸Šä¼ : {self.uploader.upload_count} ä¸ªIP")
            else:
                # ğŸ”§ æ”¹è¿›ï¼šä¸Šä¼ å¤±è´¥ï¼Œæ•°æ®å·²æŒä¹…åŒ–ï¼Œä¸ä¼šä¸¢å¤±
                logger.error(f"âœ— ä¸Šä¼ å¤±è´¥ï¼Œæ•°æ®å·²æŒä¹…åŒ–åˆ° {PENDING_QUEUE_FILE}ï¼Œä¸‹æ¬¡é‡å¯ä¼šæ¢å¤")
                # ä¸æ¸…ç†æ•°æ®ï¼Œä¿ç•™åœ¨å†…å­˜å’Œç£ç›˜ä¸­
        finally:
            self.upload_in_progress = False
    
    async def wait_for_server_online(self) -> bool:
        """
        ç­‰å¾…æœåŠ¡å™¨åœ¨çº¿
        è¿”å›: Trueè¡¨ç¤ºæœåŠ¡å™¨åœ¨çº¿ï¼ŒFalseè¡¨ç¤ºè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°åä»ç„¶ç¦»çº¿
        """
        logger.info("æ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦åœ¨çº¿...")
        
        # æ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦åœ¨çº¿
        is_online = await self.uploader.check_server_online()
        
        if is_online:
            # æœåŠ¡å™¨åœ¨çº¿ï¼Œé‡ç½®ç¦»çº¿è®¡æ•°å™¨
            if self.server_offline_count > 0:
                logger.info(f"ğŸ‰ æœåŠ¡å™¨å·²æ¢å¤åœ¨çº¿ï¼ˆä¹‹å‰ç¦»çº¿ {self.server_offline_count} æ¬¡ï¼‰")
            self.server_offline_count = 0
            return True
        
        # æœåŠ¡å™¨ç¦»çº¿ï¼Œå¢åŠ è®¡æ•°å™¨
        self.server_offline_count += 1
        
        # è®¡ç®—ç­‰å¾…æ—¶é—´ï¼ˆéšç€ç¦»çº¿æ¬¡æ•°å¢åŠ è€Œå¢åŠ ç­‰å¾…æ—¶é—´ï¼Œä½†æœ‰ä¸Šé™ï¼‰
        if self.server_offline_count <= MAX_SERVER_CHECK_RETRIES:
            wait_time = SERVER_CHECK_INTERVAL
        else:
            # è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°åï¼Œä½¿ç”¨æ›´é•¿çš„ç­‰å¾…æ—¶é—´
            wait_time = SERVER_CHECK_INTERVAL * 2
        
        logger.warning(
            f" æœåŠ¡å™¨ç¦»çº¿ï¼ˆç¬¬ {self.server_offline_count} æ¬¡æ£€æµ‹ï¼‰ï¼Œ"
            f"å°†åœ¨ {wait_time} ç§’åé‡è¯•..."
        )
        
        await asyncio.sleep(wait_time)
        return False
    
    async def run_once(self):
        """æ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„å¤„ç†æµç¨‹"""
        logger.info("-" * 80)
        logger.info("å¼€å§‹æ‰§è¡Œæ—¥å¿—å¤„ç†ä»»åŠ¡")
        
        try:
            # 1. å¤„ç†æ—¥å¿—æ–‡ä»¶å¹¶è¾¹è¯»è¾¹ä¼ 
            await self.process_log_files_and_upload()
            
            # 2. ä¸Šä¼ å‰©ä½™çš„IPï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            await self.upload_new_ips()
            
            # 3. æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            self.print_stats()
            
        except Exception as e:
            logger.error(f"å¤„ç†ä»»åŠ¡å¼‚å¸¸: {e}")
        
        logger.info("ä»»åŠ¡æ‰§è¡Œå®Œæˆ")
    
    async def run_forever(self):
        """æŒç»­è¿è¡Œå¤„ç†å™¨"""
        logger.info("=" * 80)
        logger.info("å¼‚æ­¥æ—¥å¿—å¤„ç†å™¨å¯åŠ¨")
        logger.info("=" * 80)
        logger.info(f"ç›®æ ‡æœåŠ¡å™¨: {API_ENDPOINT}")
        logger.info(f"åƒµå°¸ç½‘ç»œç±»å‹: {BOTNET_TYPE}")
        logger.info(f"æ—¥å¿—ç›®å½•: {LOG_DIR}")
        logger.info(f"æ–‡ä»¶æ¨¡å¼: {LOG_FILE_PATTERN}")
        logger.info(f"å¤„ç†é—´éš”: {UPLOAD_INTERVAL} ç§’")
        logger.info(f"æ‰¹é‡å¤§å°: {BATCH_SIZE} ä¸ªIP")
        logger.info(f"æœåŠ¡å™¨æ£€æŸ¥é—´éš”: {SERVER_CHECK_INTERVAL} ç§’")
        logger.info("=" * 80)
        
        try:
            while True:
                # 1. å…ˆæ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦åœ¨çº¿
                server_online = await self.wait_for_server_online()
                
                if not server_online:
                    # æœåŠ¡å™¨ç¦»çº¿ï¼Œè·³è¿‡æœ¬æ¬¡å¤„ç†ï¼Œç»§ç»­ç­‰å¾…
                    continue
                
                # 2. æœåŠ¡å™¨åœ¨çº¿ï¼Œæ‰§è¡Œæ­£å¸¸çš„å¤„ç†æµç¨‹
                await self.run_once()
                
                # 3. ç­‰å¾…ä¸‹æ¬¡æ‰§è¡Œ
                logger.info(f"ç­‰å¾… {UPLOAD_INTERVAL} ç§’åæ‰§è¡Œä¸‹æ¬¡å¤„ç†...")
                await asyncio.sleep(UPLOAD_INTERVAL)
                
        except KeyboardInterrupt:
            logger.info("\næ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨é€€å‡º...")
        except Exception as e:
            logger.error(f"å¤„ç†å™¨å¼‚å¸¸: {e}")
        finally:
            # æ¸…ç†èµ„æº
            await self.uploader.close_session()
            
            # æœ€ç»ˆç»Ÿè®¡
            logger.info("=" * 80)
            logger.info("å¤„ç†å™¨å·²åœæ­¢")
            self.print_stats()
            logger.info("=" * 80)
    
    def print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¢å¼ºç‰ˆ - æ›´è¯¦ç»†çš„ç›‘æ§ï¼‰"""
        ip_stats = self.ip_processor.get_stats()
        
        logger.info("=" * 80)
        logger.info("ğŸ“Š å¤„ç†ç»Ÿè®¡æŠ¥å‘Š")
        logger.info("=" * 80)
        
        # åŸºç¡€ç»Ÿè®¡
        logger.info("ğŸ“ æ•°æ®å¤„ç†:")
        logger.info(f"  â”œâ”€ å·²å¤„ç†è¡Œæ•°: {ip_stats['processed_lines']:,}")
        logger.info(f"  â”œâ”€ æ—¥å†…é‡å¤IP: {ip_stats['duplicate_count']:,}")
        logger.info(f"  â”œâ”€ å…¨å±€é‡å¤IP: {ip_stats.get('global_duplicate_count', 0):,}")
        logger.info(f"  â””â”€ å”¯ä¸€IPæ€»æ•°: {ip_stats.get('unique_ips_total', 0):,}")
        
        # ä¸Šä¼ ç»Ÿè®¡
        logger.info("")
        logger.info("ğŸ“¤ ä¸Šä¼ çŠ¶æ€:")
        logger.info(f"  â”œâ”€ å¾…ä¸Šä¼ IPæ•°: {ip_stats['new_ips_pending']:,}")
        logger.info(f"  â”œâ”€ å·²ç¼“å­˜IPæ•°: {ip_stats['cached_ips']:,}")
        logger.info(f"  â”œâ”€ ç´¯è®¡ä¸Šä¼ æ•°: {self.uploader.upload_count:,}")
        logger.info(f"  â””â”€ å¤±è´¥æ¬¡æ•°: {self.uploader.error_count}")
        
        # å»é‡æ•ˆç‡
        if ip_stats['processed_lines'] > 0:
            total_duplicates = ip_stats['duplicate_count'] + ip_stats.get('global_duplicate_count', 0)
            dedup_rate = (total_duplicates / ip_stats['processed_lines']) * 100
            logger.info("")
            logger.info("ğŸ¯ å»é‡æ•ˆç‡:")
            logger.info(f"  â””â”€ å»é‡ç‡: {dedup_rate:.1f}% ({total_duplicates:,} / {ip_stats['processed_lines']:,})")
        
        # å†…å­˜å’Œç£ç›˜çŠ¶æ€
        logger.info("")
        logger.info("ğŸ’¾ å­˜å‚¨çŠ¶æ€:")
        
        # æŒä¹…åŒ–é˜Ÿåˆ—æ£€æŸ¥
        if os.path.exists(PENDING_QUEUE_FILE):
            try:
                queue_size = os.path.getsize(PENDING_QUEUE_FILE) / 1024 / 1024
                logger.info(f"  â”œâ”€ é˜Ÿåˆ—æ–‡ä»¶: {queue_size:.2f} MB")
            except:
                logger.info(f"  â”œâ”€ é˜Ÿåˆ—æ–‡ä»¶: å­˜åœ¨")
        else:
            logger.info(f"  â”œâ”€ é˜Ÿåˆ—æ–‡ä»¶: æ— ")
        
        # ä¸å®Œæ•´è¡Œç¼“å­˜
        incomplete_count = len(self.log_reader.incomplete_lines)
        if incomplete_count > 0:
            logger.info(f"  â”œâ”€ ä¸å®Œæ•´è¡Œ: {incomplete_count} ä¸ªæ–‡ä»¶")
        
        # æ–‡ä»¶ç¼“å­˜
        if self.log_reader.file_cache:
            logger.info(f"  â””â”€ æ–‡ä»¶ç¼“å­˜: {len(self.log_reader.file_cache)} ä¸ªæ–‡ä»¶")
        
        # å†…å­˜å‹åŠ›å‘Šè­¦
        logger.info("")
        if ip_stats['new_ips_pending'] > FORCE_UPLOAD_THRESHOLD:
            logger.warning(f"âš ï¸  å†…å­˜å‹åŠ›è­¦å‘Š: å¾…ä¸Šä¼ æ•°æ® {ip_stats['new_ips_pending']:,} è¶…è¿‡é˜ˆå€¼ {FORCE_UPLOAD_THRESHOLD:,}")
        elif ip_stats['new_ips_pending'] > MIN_UPLOAD_BATCH:
            logger.info(f"â„¹ï¸  å¾…ä¸Šä¼ æ•°æ®: {ip_stats['new_ips_pending']:,} æ¡ (æ­£å¸¸èŒƒå›´)")
        else:
            logger.info(f"âœ“  å¾…ä¸Šä¼ æ•°æ®: {ip_stats['new_ips_pending']:,} æ¡ (å¥åº·)")
        
        logger.info("=" * 80)


# ============================================================
# å‘½ä»¤è¡Œæ¨¡å¼
# ============================================================

async def test_connection():
    """æµ‹è¯•è¿æ¥"""
    print("æµ‹è¯•è¿æ¥åˆ°æœ¬åœ°æœåŠ¡å™¨...")
    print(f"ç›®æ ‡: {API_ENDPOINT}")
    
    try:
        timeout = aiohttp.ClientTimeout(total=10)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            status_url = f"http://{LOCAL_SERVER_HOST}:{LOCAL_SERVER_PORT}/api/upload-status"
            
            async with session.get(status_url) as response:
                if response.status == 200:
                    data = await response.json()
                    print(f" è¿æ¥æˆåŠŸ!")
                    print(f"æœåŠ¡å™¨çŠ¶æ€: {data.get('api_status', 'unknown')}")
                    print(f"æœåŠ¡å™¨æ—¶é—´: {data.get('timestamp', 'unknown')}")
                    return True
                else:
                    print(f" è¿æ¥å¤±è´¥: HTTP {response.status}")
                    return False
                    
    except Exception as e:
        print(f" è¿æ¥å¤±è´¥: {e}")
        return False


async def test_upload():
    """æµ‹è¯•ä¸Šä¼ åŠŸèƒ½"""
    print("\næµ‹è¯•ä¸Šä¼ åŠŸèƒ½...")
    
    # æ¨¡æ‹ŸIPæ•°æ®
    test_ip_data = [{
        'ip': '1.2.3.4',
        'date': datetime.now().strftime('%Y-%m-%d'),
        'botnet_type': BOTNET_TYPE,
        'timestamp': datetime.now().isoformat()
    }]
    
    uploader = RemoteUploader()
    success = await uploader.upload_ips(test_ip_data)
    await uploader.close_session()
    
    if success:
        print(" ä¸Šä¼ æµ‹è¯•æˆåŠŸ!")
        return True
    else:
        print(" ä¸Šä¼ æµ‹è¯•å¤±è´¥!")
        return False


async def test_log_processing():
    """æµ‹è¯•æ—¥å¿—å¤„ç†åŠŸèƒ½"""
    print("\næµ‹è¯•æ—¥å¿—å¤„ç†åŠŸèƒ½...")
    
    processor = AsyncLogProcessor()
    
    # æ£€æŸ¥æ—¥å¿—æ–‡ä»¶
    log_files = processor.log_reader.get_available_log_files()
    if not log_files:
        print(" æ²¡æœ‰æ‰¾åˆ°æ—¥å¿—æ–‡ä»¶")
        print(f"è¯·æ£€æŸ¥æ—¥å¿—ç›®å½•: {LOG_DIR}")
        print(f"æ–‡ä»¶æ¨¡å¼: {LOG_FILE_PATTERN}")
        return False
    
    print(f" æ‰¾åˆ° {len(log_files)} ä¸ªæ—¥å¿—æ–‡ä»¶")
    for date, path in log_files:
        file_size = path.stat().st_size if path.exists() else 0
        print(f"  {date.strftime('%Y-%m-%d')}: {path} ({file_size:,} bytes)")
    
    return True


async def main_async():
    """å¼‚æ­¥ä¸»å‡½æ•°"""
    if len(sys.argv) > 1:
        command = sys.argv[1]
        
        if command == "test":
            # æµ‹è¯•æ¨¡å¼
            print("=" * 80)
            print("  è¿œç«¯ä¸Šä¼ å™¨ - æµ‹è¯•æ¨¡å¼")
            print("=" * 80)
            
            # æµ‹è¯•è¿æ¥
            conn_ok = await test_connection()
            if not conn_ok:
                print("\nè¯·æ£€æŸ¥:")
                print("  1. LOCAL_SERVER_HOST æ˜¯å¦æ­£ç¡®ï¼Ÿ")
                print("  2. æœ¬åœ°æœåŠ¡å™¨æ˜¯å¦è¿è¡Œï¼Ÿ")
                print("  3. é˜²ç«å¢™æ˜¯å¦å¼€æ”¾ç«¯å£ï¼Ÿ")
                sys.exit(1)
            
            # æµ‹è¯•ä¸Šä¼ 
            upload_ok = await test_upload()
            if not upload_ok:
                print("\nè¯·æ£€æŸ¥:")
                print("  1. API_KEY æ˜¯å¦æ­£ç¡®ï¼Ÿ")
                print("  2. è¿œç«¯IPæ˜¯å¦åœ¨ç™½åå•ä¸­ï¼Ÿ")
                sys.exit(1)
            
            # æµ‹è¯•æ—¥å¿—å¤„ç†
            log_ok = await test_log_processing()
            if not log_ok:
                print("\nè¯·æ£€æŸ¥:")
                print("  1. LOG_DIR è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼Ÿ")
                print("  2. æ—¥å¿—æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Ÿ")
                print("  3. LOG_FILE_PATTERN æ¨¡å¼æ˜¯å¦æ­£ç¡®ï¼Ÿ")
                sys.exit(1)
            
            print("\n æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å¯ä»¥è¿è¡Œæ­£å¼æ¨¡å¼")
            print(f"\nå¯åŠ¨å‘½ä»¤: python {sys.argv[0]}")
            
        elif command == "once":
            # å•æ¬¡æ‰§è¡Œ
            processor = AsyncLogProcessor()
            await processor.run_once()
            await processor.uploader.close_session()
            
        else:
            print(f"æœªçŸ¥å‘½ä»¤: {command}")
            print(f"ç”¨æ³•:")
            print(f"  python {sys.argv[0]}       - æŒç»­è¿è¡Œ")
            print(f"  python {sys.argv[0]} test  - æµ‹è¯•è¿æ¥")
            print(f"  python {sys.argv[0]} once  - å•æ¬¡æ‰§è¡Œ")
    else:
        # æŒç»­è¿è¡Œæ¨¡å¼
        processor = AsyncLogProcessor()
        await processor.run_forever()


def main():
    """ä¸»å‡½æ•°"""
    try:
        asyncio.run(main_async())
    except KeyboardInterrupt:
        logger.info("ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"ç¨‹åºå¼‚å¸¸: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()





