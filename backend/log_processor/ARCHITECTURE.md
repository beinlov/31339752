# 系统架构文档

## 整体架构

```
┌─────────────────────────────────────────────────────────────┐
│                    远端僵尸网络蜜罐/传感器                      │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐    │
│  │ Asruex   │  │   Mozi   │  │Andromeda │  │  Moobot  │    │
│  │   C2     │  │  Honeypot│  │ Honeypot │  │ Honeypot │    │
│  └────┬─────┘  └────┬─────┘  └────┬─────┘  └────┬─────┘    │
└───────┼─────────────┼─────────────┼─────────────┼───────────┘
        │             │             │             │
        │  SSH/SFTP/HTTP/rsync 日志传输            │
        ↓             ↓             ↓             ↓
┌─────────────────────────────────────────────────────────────┐
│              本地日志接收目录 (logs/)                          │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐    │
│  │ asruex/  │  │  mozi/   │  │andromeda/│  │ moobot/  │    │
│  │*.txt     │  │*.txt     │  │*.txt     │  │*.txt     │    │
│  └────┬─────┘  └────┬─────┘  └────┬─────┘  └────┬─────┘    │
└───────┼─────────────┼─────────────┼─────────────┼───────────┘
        │             │             │             │
        │         watchdog 文件监控                │
        ↓             ↓             ↓             ↓
┌─────────────────────────────────────────────────────────────┐
│         统一日志处理引擎 (log_processor/)                     │
│  ┌──────────────────────────────────────────────────────┐   │
│  │  watcher.py - 文件监控                                │   │
│  │  • 监控所有僵尸网络日志目录                            │   │
│  │  • 检测文件创建和修改事件                              │   │
│  │  • 记录文件读取位置（断点续传）                        │   │
│  └──────────────┬───────────────────────────────────────┘   │
│                 ↓                                            │
│  ┌──────────────────────────────────────────────────────┐   │
│  │  parser.py - 日志解析                                 │   │
│  │  • 解析CSV格式: timestamp,ip,event_type,extras        │   │
│  │  • 验证IP地址格式                                      │   │
│  │  • 过滤重要事件                                        │   │
│  └──────────────┬───────────────────────────────────────┘   │
│                 ↓                                            │
│  ┌──────────────────────────────────────────────────────┐   │
│  │  enricher.py - IP信息增强                             │   │
│  │  • 调用 ip_query() 查询地理位置                       │   │
│  │  • 获取 ISP、ASN、经纬度                               │   │
│  │  • 实现缓存机制（10000条，24小时TTL）                 │   │
│  └──────────────┬───────────────────────────────────────┘   │
│                 ↓                                            │
│  ┌──────────────────────────────────────────────────────┐   │
│  │  db_writer.py - 数据库写入                            │   │
│  │  • 批量写入（默认100条）                               │   │
│  │  • 自动创建表结构                                      │   │
│  │  • 定期提交（60秒间隔）                                │   │
│  └──────────────┬───────────────────────────────────────┘   │
└─────────────────┼───────────────────────────────────────────┘
                  ↓
┌─────────────────────────────────────────────────────────────┐
│                      MySQL 数据库                             │
│  ┌──────────────────────────────────────────────────────┐   │
│  │  botnet_nodes_{type}     - 节点详细信息表             │   │
│  │  • IP、地理位置、ISP、ASN、经纬度                      │   │
│  │  • 状态、最后活跃时间、是否中国                        │   │
│  └──────────────────────────────────────────────────────┘   │
│  ┌──────────────────────────────────────────────────────┐   │
│  │  china_botnet_{type}     - 中国统计表                 │   │
│  │  • 省份、城市、感染数量                                │   │
│  └──────────────────────────────────────────────────────┘   │
│  ┌──────────────────────────────────────────────────────┐   │
│  │  global_botnet_{type}    - 全球统计表                 │   │
│  │  • 国家、感染数量                                      │   │
│  └──────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────┘
```

## 核心模块详解

### 1. config.py - 配置管理

**职责：**
- 集中管理所有配置项
- 定义僵尸网络类型和特征
- 设置性能参数

**关键配置：**
```python
BOTNET_CONFIG = {
    'asruex': {
        'log_dir': '日志目录',
        'important_events': ['需要保存的事件类型'],
        'enabled': True,
        'description': '描述'
    }
}

DB_CONFIG = {数据库连接配置}
IP_CACHE_SIZE = 10000      # IP缓存大小
DB_BATCH_SIZE = 100        # 批量写入大小
DB_COMMIT_INTERVAL = 60    # 提交间隔（秒）
```

### 2. parser.py - 日志解析器

**职责：**
- 解析统一CSV格式日志
- 验证数据有效性
- 提取关键信息

**核心方法：**
```python
parse_line(line)           # 解析单行日志
should_save_to_db(data)    # 判断是否保存
extract_command(data)      # 提取命令（asruex）
get_description(data)      # 获取事件描述
```

**日志格式：**
```
timestamp,ip,event_type,extra1,extra2,...
```

### 3. enricher.py - IP信息增强器

**职责：**
- 查询IP地理位置信息
- 实现智能缓存
- 支持批量查询

**核心方法：**
```python
enrich(ip)                 # 增强单个IP
batch_enrich(ips)          # 批量增强
get_stats()                # 获取统计信息
clear_cache()              # 清空缓存
```

**数据来源：**
- `ip_location/ip_query.py` - 使用AWDB数据库
- 返回：国家、省、市、ISP、ASN、经纬度等

**性能优化：**
- LRU缓存（10000条）
- 24小时TTL
- 异步批量查询

### 4. db_writer.py - 数据库写入器

**职责：**
- 批量写入节点数据
- 自动创建表结构
- 维护统计信息

**核心方法：**
```python
add_node(log_data, ip_info)    # 添加节点到缓冲区
flush(force=False)             # 刷新缓冲区
update_statistics()            # 更新统计表
get_stats()                    # 获取统计信息
```

**表结构：**

**botnet_nodes_{type}:**
```sql
CREATE TABLE botnet_nodes_asruex (
    id INT AUTO_INCREMENT PRIMARY KEY,
    ip VARCHAR(15) NOT NULL,
    longitude FLOAT,
    latitude FLOAT,
    country VARCHAR(50),
    province VARCHAR(50),
    city VARCHAR(50),
    continent VARCHAR(50),
    isp VARCHAR(255),
    asn VARCHAR(50),
    status ENUM('active', 'inactive'),
    last_active TIMESTAMP,
    created_at TIMESTAMP,
    updated_at TIMESTAMP,
    is_china BOOLEAN,
    INDEX idx_ip (ip),
    INDEX idx_location (country, province, city),
    INDEX idx_status (status)
);
```

### 5. watcher.py - 日志监控器

**职责：**
- 实时监控文件变化
- 记录读取位置
- 触发处理流程

**核心类：**

**BotnetLogHandler:**
- 处理单个僵尸网络的文件事件
- 记录文件位置（`.file_positions.json`）
- 增量读取新内容

**BotnetLogWatcher:**
- 管理所有僵尸网络的监控器
- 启动/停止监控
- 扫描已存在文件

**事件处理流程：**
```
文件变化 → on_modified/on_created 
        → _process_file 
        → callback(botnet_type, line)
        → 保存位置状态
```

### 6. main.py - 主程序

**职责：**
- 整合所有模块
- 管理生命周期
- 定期任务调度

**核心类：**

**BotnetLogProcessor:**
```python
__init__()                     # 初始化所有组件
process_log_line(type, line)   # 处理单行日志
start()                        # 启动服务
stop()                         # 停止服务
_periodic_flush()              # 定期刷新任务
_print_stats()                 # 打印统计信息
```

**处理流程：**
```
1. 初始化解析器、增强器、写入器
2. 创建文件监控器
3. 启动监控
4. 处理已存在文件
5. 启动定期刷新任务
6. 进入事件循环
```

## 数据流转

### 完整处理流程

```
1. 远端蜜罐生成日志
   ↓
2. 通过SSH/rsync传输到本地logs/目录
   ↓
3. watchdog检测到文件变化
   ↓
4. watcher读取新增内容
   ↓
5. parser解析CSV格式
   ↓
6. parser判断是否为重要事件
   ↓ (是)
7. enricher查询IP信息（先查缓存）
   ↓
8. enricher返回完整IP信息
   ↓
9. db_writer添加到缓冲区
   ↓
10. 达到batch_size或时间间隔
   ↓
11. db_writer批量写入数据库
   ↓
12. 更新统计信息
```

### 状态管理

**文件位置记录：**
```json
{
  "asruex": {
    "/path/to/logs/asruex/2025-10-29.txt": 12345
  },
  "mozi": {
    "/path/to/logs/mozi/2025-10-29.txt": 6789
  }
}
```

**缓存管理：**
- IP查询结果缓存在内存中
- LRU策略，超过size自动淘汰
- TTL过期自动清理

**批量缓冲：**
- 节点数据缓冲在内存
- 达到batch_size触发写入
- 定期timer强制刷新

## 性能特性

### 优化策略

1. **IP查询缓存**
   - 减少重复查询
   - 典型命中率：70-90%
   - 内存占用：约10-20MB

2. **批量数据库写入**
   - 减少数据库连接
   - 提高写入效率
   - 批量大小可配置

3. **异步处理**
   - 非阻塞IO
   - 并发处理多个文件
   - 批量IP查询

4. **断点续传**
   - 记录文件位置
   - 重启后继续处理
   - 避免重复处理

### 性能指标

**处理能力：**
- 单线程：~1000行/秒
- 批量模式：~5000行/秒
- IP查询（有缓存）：~10000次/秒
- IP查询（无缓存）：~100次/秒

**资源占用：**
- 内存：50-200MB（取决于缓存大小）
- CPU：5-20%（空闲时<5%）
- 磁盘IO：低（仅读日志和写数据库）

## 扩展性设计

### 添加新僵尸网络

只需3步：

1. **配置**
```python
# config.py
'new_botnet': {
    'log_dir': os.path.join(LOGS_DIR, 'new_botnet'),
    'important_events': ['infection', 'beacon'],
    'enabled': True
}
```

2. **创建目录**
```bash
mkdir logs/new_botnet
```

3. **传输日志**
```bash
rsync remote:/logs/ logs/new_botnet/
```

### 自定义解析逻辑

继承 `LogParser` 类：

```python
class CustomParser(LogParser):
    def parse_line(self, line):
        # 自定义解析逻辑
        pass
```

### 自定义写入逻辑

继承 `BotnetDBWriter` 类：

```python
class CustomWriter(BotnetDBWriter):
    async def _insert_nodes(self, cursor, nodes):
        # 自定义写入逻辑
        pass
```

## 容错机制

### 错误处理

1. **解析错误**
   - 记录错误日志
   - 跳过无效行
   - 继续处理后续内容

2. **IP查询失败**
   - 返回默认值
   - 记录警告日志
   - 不阻塞流程

3. **数据库写入失败**
   - 回滚事务
   - 记录错误日志
   - 保留缓冲区数据
   - 下次flush重试

### 重启恢复

1. 读取 `.file_positions.json`
2. 从上次位置继续读取
3. 恢复处理流程

### 监控和告警

建议配置：
- 日志监控（错误关键词）
- 数据库写入监控
- 缓冲区大小监控
- 处理速度监控

## 安全考虑

### 输入验证

- IP地址格式验证
- SQL注入防护（使用参数化查询）
- 文件路径验证

### 权限控制

- 数据库最小权限原则
- 日志目录访问控制
- 配置文件保护

### 日志脱敏

根据需要实现：
```python
def sanitize_ip(ip):
    """脱敏IP地址"""
    parts = ip.split('.')
    return f"{parts[0]}.{parts[1]}.*.* "
```

## 总结

这是一个**模块化、可扩展、高性能**的僵尸网络日志处理系统：

✅ **统一架构** - 所有僵尸网络使用相同处理流程
✅ **实时处理** - 文件监控，即时响应
✅ **高性能** - 缓存、批量处理、异步IO
✅ **可靠性** - 断点续传、错误恢复
✅ **易扩展** - 添加新僵尸网络仅需配置
✅ **易维护** - 清晰的模块划分和文档





