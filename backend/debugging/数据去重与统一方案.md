# 节点数据去重与统一方案

## 问题现状

三个平台显示的节点数不一致：
- **后台管理系统（节点分布界面）**：76,741
- **清除界面**：76,186
- **僵尸网络展示处置平台**：76,505

差异范围：约555个节点（0.7%）

## 问题原因分析

### 1. 数据库层面
- 原始表 `botnet_nodes_{type}` 存在重复IP记录
- 同一个IP可能有多条记录（不同时间戳）
- 估计重复率约30%

### 2. 接口层面
虽然聚合器已使用 `COUNT(DISTINCT ip)`，但三个平台使用不同接口：

| 平台 | 使用接口 | 数据源 | 统计逻辑 |
|------|---------|--------|---------|
| 节点分布界面 | `/api/node-stats/{type}` | 聚合表 | SUM(infected_num) |
| 清除界面 | `/api/node-details` | 原始表 + 聚合表 | 混合统计 |
| 展示处置平台 | `/api/botnet-node-stats/{name}` | 聚合表 | SUM(infected_num) |

### 3. 数据同步问题
- 聚合表可能未及时更新
- 不同时间点的聚合结果不同
- 聚合器运行时机不同步

## 解决方案

### 步骤1：分析重复数据（必须）

```bash
cd d:\workspace\botnet\backend\scripts

# 分析所有僵尸网络的重复情况
python deduplicate_nodes.py --analyze-only

# 分析特定僵尸网络
python deduplicate_nodes.py --analyze-only --botnet ramnit
```

**输出示例：**
```
📊 总记录数: 108,960
📊 唯一IP数: 76,186
📊 重复记录: 32,774 (30.08%)

🔍 重复次数最多的IP（Top 10）:
   - IP: 192.168.1.1    重复 15 次
   - IP: 10.0.0.5       重复 12 次
   ...
```

### 步骤2：模拟去重（推荐）

```bash
# 模拟运行（不实际删除数据）
python deduplicate_nodes.py

# 模拟特定僵尸网络
python deduplicate_nodes.py --botnet ramnit
```

这会显示将要删除的记录，但不会实际修改数据库。

### 步骤3：执行去重（谨慎）

**⚠️ 警告：此操作会永久删除数据，请先备份数据库！**

```bash
# 备份数据库（推荐）
mysqldump -u root -p botnet_monitor > backup_$(date +%Y%m%d_%H%M%S).sql

# 执行去重（所有僵尸网络）
python deduplicate_nodes.py --execute

# 执行去重（特定僵尸网络）
python deduplicate_nodes.py --execute --botnet ramnit
```

**去重策略：**
- 保留每个IP的最新记录（按 `updated_at` 降序，`id` 降序）
- 删除同一IP的旧记录
- 分批删除，每批1000条

### 步骤4：重建聚合表（必须）

去重完成后，必须重建聚合表以确保统计一致：

```bash
cd d:\workspace\botnet\backend\scripts

# 重建所有聚合表
python rebuild_aggregation.py

# 重建特定僵尸网络的聚合表
python rebuild_aggregation.py --botnet ramnit
```

**操作内容：**
1. 清空旧的聚合表
2. 按IP去重重新统计
3. 验证数据一致性

### 步骤5：验证数据一致性

运行以下SQL验证：

```sql
-- 检查原始表去重后的数量
SELECT COUNT(DISTINCT ip) as unique_ips 
FROM botnet_nodes_ramnit;

-- 检查全球聚合表总数
SELECT SUM(infected_num) as total 
FROM global_botnet_ramnit;

-- 检查中国聚合表总数
SELECT SUM(infected_num) as total 
FROM china_botnet_ramnit;

-- 三者应该相等或非常接近
```

## 预期结果

### 去重前
- 原始表记录：108,960
- 唯一IP：76,186
- 重复记录：32,774（30%）

### 去重后
- 原始表记录：76,186（减少30%）
- 唯一IP：76,186
- 重复记录：0

### 聚合表重建后
所有平台显示相同的节点数：**约76,186**（具体数值取决于地理位置标注情况）

## 完整操作流程

### 推荐流程（安全）

```bash
# 1. 进入脚本目录
cd d:\workspace\botnet\backend\scripts

# 2. 分析重复情况
python deduplicate_nodes.py --analyze-only

# 3. 模拟去重
python deduplicate_nodes.py

# 4. 备份数据库（重要！）
# 使用MySQL客户端或数据库管理工具备份

# 5. 执行去重
python deduplicate_nodes.py --execute

# 6. 重建聚合表
python rebuild_aggregation.py

# 7. 验证结果
# 使用SQL查询验证数据一致性
```

### 快速流程（已备份）

```bash
cd d:\workspace\botnet\backend\scripts

# 一键执行：去重 + 重建聚合表
python deduplicate_nodes.py --execute && python rebuild_aggregation.py
```

## 后续维护

### 1. 防止重复数据产生

建议在原始表添加唯一索引：

```sql
-- 为每个僵尸网络表添加IP唯一索引
ALTER TABLE botnet_nodes_ramnit 
ADD UNIQUE INDEX idx_unique_ip (ip);

-- 如果IP+时间戳组合唯一，可以用：
ALTER TABLE botnet_nodes_ramnit 
ADD UNIQUE INDEX idx_unique_ip_time (ip, updated_at);
```

**注意：** 添加唯一索引前必须先去重，否则会失败。

### 2. 定期运行聚合器

设置定时任务，定期更新聚合表：

```bash
# Linux crontab
*/5 * * * * cd /path/to/backend/stats_aggregator && python incremental_aggregator.py

# Windows 任务计划程序
# 每5分钟运行一次 incremental_aggregator.py
```

### 3. 数据一致性监控

创建监控脚本，定期检查三个平台的数据是否一致：

```python
# monitor_consistency.py
import requests

# 获取三个平台的数据
node_stats = requests.get('http://localhost:8000/api/node-stats/ramnit').json()
node_details = requests.get('http://localhost:8000/api/node-details?botnet_type=ramnit&page=1&page_size=1').json()
botnet_stats = requests.get('http://localhost:8000/api/botnet-node-stats/ramnit').json()

# 比较
total1 = node_stats['data']['total_nodes']
total2 = node_details['data']['pagination']['total_count']
total3 = botnet_stats['total_nodes']

if total1 == total2 == total3:
    print(f"✅ 数据一致: {total1}")
else:
    print(f"❌ 数据不一致:")
    print(f"   节点分布: {total1}")
    print(f"   清除界面: {total2}")
    print(f"   展示平台: {total3}")
```

## 技术细节

### 去重SQL逻辑

```sql
-- 查找需要删除的记录（保留最新的）
SELECT t1.id, t1.ip, t1.updated_at
FROM botnet_nodes_ramnit t1
WHERE EXISTS (
    SELECT 1 
    FROM botnet_nodes_ramnit t2 
    WHERE t2.ip = t1.ip 
    AND (
        t2.updated_at > t1.updated_at 
        OR (t2.updated_at = t1.updated_at AND t2.id > t1.id)
    )
)
ORDER BY t1.ip, t1.updated_at;
```

### 聚合SQL逻辑

```sql
-- 中国统计（按IP去重）
SELECT 
    province,
    municipality,
    COUNT(DISTINCT ip) as infected_num
FROM botnet_nodes_ramnit
WHERE country = '中国'
GROUP BY province, municipality;

-- 全球统计（按IP去重）
SELECT 
    country,
    COUNT(DISTINCT ip) as infected_num
FROM botnet_nodes_ramnit
GROUP BY country;
```

## 故障排查

### 问题1：去重后数据仍不一致

**可能原因：**
- 聚合表未重建
- 有新数据插入

**解决方案：**
```bash
# 重新运行聚合器
python rebuild_aggregation.py
```

### 问题2：删除失败

**可能原因：**
- 权限不足
- 外键约束

**解决方案：**
```sql
-- 检查外键约束
SHOW CREATE TABLE botnet_nodes_ramnit;

-- 临时禁用外键检查（谨慎）
SET FOREIGN_KEY_CHECKS = 0;
-- 执行删除
SET FOREIGN_KEY_CHECKS = 1;
```

### 问题3：性能问题

**可能原因：**
- 表数据量大
- 索引缺失

**解决方案：**
```sql
-- 添加索引加速去重
CREATE INDEX idx_ip_updated ON botnet_nodes_ramnit(ip, updated_at DESC);

-- 去重后可以删除
DROP INDEX idx_ip_updated ON botnet_nodes_ramnit;
```

## 预计耗时

| 操作 | 预计时间 | 说明 |
|------|---------|------|
| 分析重复数据 | 1-5分钟 | 取决于数据量 |
| 模拟去重 | 1-3分钟 | 仅查询，不删除 |
| 执行去重 | 5-30分钟 | 100万记录约需10分钟 |
| 重建聚合表 | 2-10分钟 | 包含统计和验证 |
| **总计** | **10-50分钟** | 建议在低峰期执行 |

## 风险评估

| 风险 | 等级 | 应对措施 |
|------|------|---------|
| 数据丢失 | 🔴 高 | **必须先备份数据库** |
| 服务中断 | 🟡 中 | 在低峰期执行，或使用只读副本 |
| 数据不一致 | 🟢 低 | 执行后验证，可回滚 |
| 性能影响 | 🟡 中 | 分批处理，添加索引 |

## 回滚方案

如果去重后发现问题：

```bash
# 1. 恢复数据库备份
mysql -u root -p botnet_monitor < backup_20241211_150000.sql

# 2. 重新运行聚合器
cd backend/scripts
python rebuild_aggregation.py

# 3. 验证数据
# 使用SQL查询验证
```

## 支持与帮助

遇到问题时的检查清单：

- [ ] 是否已备份数据库？
- [ ] 是否在正确的目录执行脚本？
- [ ] 是否有数据库操作权限？
- [ ] 是否在低峰期执行？
- [ ] 去重后是否运行了聚合器？
- [ ] 是否验证了数据一致性？

---

**创建时间：** 2024-12-11  
**版本：** 1.0  
**作者：** Cascade AI Assistant
