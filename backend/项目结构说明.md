# 僵尸网络接管平台 - 后端项目结构说明

## 📖 快速索引

### 🚀 如果你想...

- **启动整个系统**: 运行根目录的 `start_all_services.bat` (Windows) 或 `start_all_services.sh` (Linux/Mac)
- **查看核心模块**: 跳转到 [核心模块](#-核心模块) 章节
- **了解日志处理**: 查看 [log_processor](#4-log_processor---日志处理核心模块-) 章节
- **理解数据流**: 查看 [数据流架构图](#-数据流架构图)
- **查找测试脚本**: 查看 [test/](#-test---测试代码目录) 章节
- **了解废弃代码**: 查看 [old/](#-old---废弃代码存档) 章节

---

## 📁 目录结构概览

```
backend/
├── main.py                 # FastAPI主应用入口
├── config.py              # 全局配置文件
├── router/                # API路由模块
├── log_processor/         # 日志处理核心模块 ⭐核心
├── stats_aggregator/      # 数据统计聚合模块 ⭐核心
├── ip_location/           # IP地理位置查询模块
├── logs/                  # 日志文件存储目录
├── test/                  # 测试代码目录
├── old/                   # 废弃代码存档
└── utils/                 # 工具函数模块
```

**启动脚本位置**: 项目根目录
- ✅ `start_all_services.bat/sh` - 启动所有服务(推荐)
- ✅ `stop_all_services.sh` - 停止所有服务

---

## 🚀 核心模块

### 1. **main.py** - FastAPI主应用
**作用**: 
- FastAPI应用的主入口文件
- 集成所有路由模块
- 提供远程日志上传API接口(`/api/upload-logs`)
- 提供各类数据查询API(省份统计、全球统计、僵尸网络表信息等)

**关键功能**:
- 日志上传接口(带API密钥验证和IP白名单)
- 僵尸网络数据表查询
- 地理位置统计数据API
- 用户事件和异常报告接口

**启动方式**:
```bash
cd backend
uvicorn main:app --host 0.0.0.0 --port 8000
```

---

### 2. **config.py** - 全局配置
**作用**:
- 统一管理数据库连接配置
- 存储API密钥和安全配置
- 定义允许的僵尸网络类型

**核心配置**:
- `DB_CONFIG`: MySQL数据库连接参数
- `API_KEY`: 远程上传日志的认证密钥
- `ALLOWED_UPLOAD_IPS`: 允许上传日志的IP白名单
- `ALLOWED_BOTNET_TYPES`: 支持的僵尸网络类型列表

---

### 3. **router/** - API路由模块
**作用**: 按功能模块组织API端点

#### 文件说明:
- **`amount.py`**: 数据统计相关API
  - 省份数据量统计
  - 全球数据量统计
  - 按时间范围查询

- **`botnet.py`**: 僵尸网络管理API
  - 僵尸网络类型管理
  - 数据表创建和检查
  - 全球僵尸网络数据更新

- **`node.py`**: 节点管理API
  - 节点信息查询
  - 节点状态更新
  - 节点数据统计

- **`user.py`**: 用户管理API
  - 用户认证
  - 权限管理
  - 用户事件记录

---

### 4. **log_processor/** - 日志处理核心模块 ⭐
**作用**: 统一处理所有僵尸网络的日志数据

这是新架构的核心模块,替代了旧系统的分散式日志处理逻辑。

#### 核心组件:

##### **main.py**
- 日志处理器主程序
- 协调各个子模块工作
- 提供启动/停止接口

##### **watcher.py** - 文件监控器
- 使用`watchdog`库监控日志目录
- 检测新日志文件和文件变化
- 实时读取新增日志行

##### **parser.py** - 日志解析器
- 解析不同格式的日志行
- 提取关键信息(时间戳、IP、事件类型)
- 支持多种僵尸网络日志格式

##### **enricher.py** - 数据增强器
- 调用IP地理位置查询
- 获取ISP、ASN信息
- 添加国家、省份、城市等信息

##### **db_writer.py** - 数据库写入器
- 批量写入数据到数据库
- 自动创建数据表
- 实现两层去重机制:
  - 应用层内存去重(基于IP+时间戳+事件类型)
  - 数据库层唯一约束去重

##### **config.py**
- 日志处理器的配置文件
- 定义日志目录路径
- 设置批量写入参数

#### 文档:
- **README.md**: 模块概述和快速入门
- **QUICKSTART.md**: 快速使用指南
- **ARCHITECTURE.md**: 架构设计文档
- **DEDUPLICATION.md**: 去重机制详解
- **SUMMARY.md**: 完整实现总结

#### 启动脚本:
- **start.sh** / **start.bat**: 启动日志处理器

**启动方式**:
```bash
cd backend/log_processor
python main.py
```

---

### 5. **stats_aggregator/** - 数据统计聚合模块 ⭐
**作用**: 定期从原始节点数据生成统计数据

#### 核心文件:

##### **aggregator.py**
- 数据聚合核心逻辑
- 从`botnet_nodes_{type}`读取数据
- 聚合到`china_botnet_{type}`和`global_botnet_{type}`表
- 自动定时运行(默认每30分钟)

##### **config.yaml**
- 聚合器配置文件
- 设置聚合间隔时间
- 配置数据库连接

##### **README.md**
- 使用说明文档

**工作原理**:
1. 每隔设定时间(如30分钟)自动执行
2. 统计中国和全球节点数据
3. 按省份、国家、行业等维度聚合
4. 写入统计表供前端展示

**启动方式**:
```bash
cd backend
python -m stats_aggregator.aggregator
# 或使用启动脚本
./start_aggregator.sh  # Linux/Mac
start_aggregator.bat   # Windows
```

---

### 6. **ip_location/** - IP地理位置查询模块
**作用**: 提供IP地址的详细地理信息查询

#### 核心文件:

##### **ip_query.py**
- IP信息查询主接口
- 调用awdb数据库查询
- 返回国家、省份、城市、经纬度、ISP、ASN等

##### **awdb/** - AWDB数据库读取模块
- 高性能IP数据库格式
- 包含全球IP地址段信息
- 提供快速查询能力

##### **IP_city_single_WGS84.awdb**
- IP地理位置数据库文件(约150MB)
- 包含全球IP地址对应的地理信息

**使用示例**:
```python
from ip_location.ip_query import query_ip_info

info = query_ip_info("8.8.8.8")
print(info)
# {
#   'country': '美国',
#   'province': '加利福尼亚州',
#   'city': '山景城',
#   'isp': 'Google',
#   'asn': 'AS15169',
#   'latitude': 37.419200,
#   'longitude': -122.057404
# }
```

---

### 7. **logs/** - 日志文件存储目录
**作用**: 集中存储从远程C2服务器上传的日志文件

#### 目录结构:
```
logs/
├── asruex/          # Asruex僵尸网络日志
├── mozi/            # Mozi僵尸网络日志
├── andromeda/       # Andromeda僵尸网络日志
├── moobot/          # Moobot僵尸网络日志
├── ramnit/          # Ramnit僵尸网络日志
└── leethozer/       # Leethozer僵尸网络日志
```

**日志格式**:
- 文件命名: `YYYY-MM-DD.txt`
- 每行格式: `[时间戳] IP地址 事件类型 [其他信息]`
- 由`log_processor`自动监控和处理

**数据流**:
1. 远程C2服务器通过HTTP POST上传日志到`/api/upload-logs`
2. API将日志写入对应的`logs/{botnet_type}/`目录
3. `log_processor`实时监控文件变化
4. 自动解析、增强、写入数据库

---

## 🧪 test/ - 测试代码目录

### 文件说明:

#### **test_processor.py**
- 测试日志处理器的各个模块
- 验证解析、增强、写入功能

#### **test_upload.py**
- 测试本地日志上传API
- 模拟远程C2上传日志

#### **remote_uploader.py**
- 模拟远程C2服务器的上传客户端
- 读取日志文件并POST到本地API
- 用于开发和测试环境

#### **test_aggregator.py**
- 测试数据聚合器功能
- 验证统计数据生成逻辑

#### **test_db.py**
- 数据库连接测试
- 验证数据表结构

#### **test_api_fix.py**
- API错误修复测试脚本
- 验证修复后的API端点

#### **quick_check.py**
- 快速检查系统各组件状态
- 一键验证所有服务是否正常运行

**使用方式**:
```bash
cd backend/test
python test_processor.py
python test_upload.py
python quick_check.py
```

---

## 📦 old/ - 废弃代码存档

### 存档内容:

#### **asbackend/**
- 旧版Asruex僵尸网络的后端服务
- 包含Flask API服务器(`as_api.py`, `as_api_main.py`)
- 已被新的统一架构替代

#### **ashttpd/**
- 旧版Asruex的HTTPD服务
- 包含日志监控脚本`logtail.py`
- 数据库辅助脚本`dbhlp_access.py`、`dbhlp_clean.py`
- IP数据库和各种shell脚本
- 已被`log_processor`模块替代

#### **log_db/**
- 早期的日志数据库测试代码
- 包含`dblog.py`、`test_dblog.py`等实验性代码

#### **asruex.py**
- 旧版Asruex路由文件
- 原在`router/`目录下
- 已不再使用

#### **data_num.py**
- 旧版数据统计脚本
- 手动执行数据聚合
- 已被`stats_aggregator`模块替代

#### **generate_ip.py**
- 旧版从Excel读取IP并写入数据库的脚本
- 已被`log_processor`的统一日志处理替代

#### **delete_pro.py**
- 数据库字段清理脚本
- 用于去除省份名称中的"省"和"市"字样
- 一次性执行的维护脚本

#### **split_botnets.py**
- 旧版数据迁移脚本
- 将`global_botnets`表拆分到各个僵尸网络的独立表
- 数据库重构时使用的一次性脚本

#### **start_all.bat / start_all.sh / stop_all.sh**
- 旧版启动/停止脚本(位于backend目录)
- 功能不完整(缺少统计聚合器)
- 已被根目录的`start_all_services.bat/sh`和`stop_all_services.sh`替代

**为什么保留**:
- 参考旧系统的实现逻辑
- 迁移过程中的数据兼容性
- 历史记录和代码追溯
- 一次性维护脚本可能需要再次使用

---

## 🔧 utils/ - 工具函数模块
**作用**: 存放通用的辅助函数和工具类

(目前为空目录,预留用于未来扩展)

可能包含:
- 日期时间处理工具
- 字符串格式化工具
- 加密解密工具
- 文件操作辅助函数

---

## 🎯 启动脚本

### **根目录启动脚本** (推荐使用)

#### **start_all_services.bat / start_all_services.sh**
**作用**: 一键启动所有后端服务(最新版本)

启动顺序:
1. FastAPI主应用(端口8000)
2. 日志处理器(log_processor)
3. 数据聚合器(stats_aggregator,每30分钟聚合一次)

**使用方式**:
```bash
# Linux/Mac
./start_all_services.sh

# Windows
start_all_services.bat
```

**特点**:
- ✅ 启动所有必需的后端服务
- ✅ 包含统计聚合器(自动定时聚合)
- ✅ 保存进程ID到pids.txt
- ✅ 提供详细的服务状态信息
- ✅ 显示日志文件位置

#### **stop_all_services.sh**
**作用**: 停止所有后端服务

**使用方式**:
```bash
# Linux/Mac
./stop_all_services.sh

# Windows (手动关闭各个cmd窗口)
```

**特点**:
- 读取pids.txt中的进程ID
- 逐个停止各个服务
- 清理PID文件

---

### **backend目录启动脚本**

#### **start_aggregator.sh / start_aggregator.bat**
**作用**: 单独启动数据聚合器

**使用方式**:
```bash
# Linux/Mac
cd backend
./start_aggregator.sh

# Windows
cd backend
start_aggregator.bat
```

**适用场景**:
- 仅需要运行数据聚合器
- 调试聚合功能
- FastAPI和日志处理器已单独启动

---

## 📂 根目录其他文件

### **教程/** 文件夹
包含各种使用指南和文档:
- `API_FIX_SUMMARY.md` - API修复总结
- `CHANGES.md` - 更新日志
- `LOG_UPLOAD_API_GUIDE.md` - 日志上传API指南
- `MIGRATION_GUIDE.md` - 系统迁移指南
- `QUICK_REFERENCE.md` - 快速参考
- `QUICK_TEST_GUIDE.md` - 快速测试指南
- `README_NEW_SYSTEM.md` - 新系统说明
- `STARTUP_GUIDE.md` - 启动指南
- 各种中文说明文档

### **leethozer/** 文件夹
Leethozer僵尸网络的C&C相关代码:
- `cnc.py` - C&C服务器脚本
- `route.sh` - 路由配置脚本

### **fronted/** 文件夹
前端React应用(注:拼写为fronted而非frontend)
- `dist/` - 编译后的生产版本
- `src/` - 源代码
- `package.json` - npm依赖配置

### **SQL文件**
- `botnet.sql` - 数据库初始结构
- `botnet1.sql` - 数据库更新/补充结构

### **Excel数据文件**
- `moobot2024.xlsx` - Moobot僵尸网络数据
- `ramnit.xlsx` - Ramnit僵尸网络数据

**注**: 这些Excel文件是旧系统使用的数据导入源,新系统已改为通过远程日志上传API接收数据。

---

## 📊 数据流架构图

```
远程C2服务器
    ↓ (HTTPS POST)
本地API (/api/upload-logs)
    ↓ (写入文件)
logs/{botnet_type}/
    ↓ (文件监控)
log_processor (watcher)
    ↓ (解析)
log_processor (parser)
    ↓ (增强)
log_processor (enricher + ip_location)
    ↓ (批量写入)
数据库 botnet_nodes_{type}
    ↓ (定时聚合)
stats_aggregator
    ↓ (统计数据)
数据库 china_botnet_{type} / global_botnet_{type}
    ↓ (API查询)
FastAPI (main.py + router/)
    ↓ (JSON响应)
前端页面
```

---

## 🔐 安全机制

### API密钥认证
- 配置在`config.py`的`API_KEY`
- 远程上传需在请求头携带: `X-API-Key`

### IP白名单
- 配置在`config.py`的`ALLOWED_UPLOAD_IPS`
- 仅允许指定IP上传日志

### 数据去重
- 应用层: 内存哈希集合去重
- 数据库层: UNIQUE约束防止重复插入

### 日志限流
- `MAX_LOGS_PER_UPLOAD`: 单次上传日志行数限制

---

## 📝 数据库表结构

### 原始节点表
- `botnet_nodes_asruex`
- `botnet_nodes_mozi`
- `botnet_nodes_andromeda`
- `botnet_nodes_moobot`
- `botnet_nodes_ramnit`
- `botnet_nodes_leethozer`

**字段**: `id`, `ip`, `country`, `province`, `city`, `isp`, `asn`, `latitude`, `longitude`, `created_at`

### 统计表(中国)
- `china_botnet_asruex`
- `china_botnet_mozi`
- ... (其他类型)

**字段**: 按省份聚合的节点数量、活跃时间等

### 统计表(全球)
- `global_botnet_asruex`
- `global_botnet_mozi`
- ... (其他类型)

**字段**: 按国家聚合的节点数量、活跃时间等

---

## 🚦 系统状态检查

使用`quick_check.py`快速检查系统状态:
```bash
cd backend/test
python quick_check.py
```

检查项目:
- ✅ 数据库连接
- ✅ FastAPI服务
- ✅ 日志处理器
- ✅ 数据聚合器
- ✅ 日志目录
- ✅ 数据表完整性

---

## 📚 相关文档

- **API接口文档**: 见根目录`教程/`文件夹
- **快速开始**: `log_processor/QUICKSTART.md`
- **架构设计**: `log_processor/ARCHITECTURE.md`
- **去重机制**: `log_processor/DEDUPLICATION.md`
- **迁移指南**: `教程/MIGRATION_GUIDE.md`

---

## 🔄 版本历史

### v2.0 (当前版本)
- ✅ 统一日志处理架构
- ✅ 自动数据聚合
- ✅ 远程日志上传API
- ✅ 两层数据去重
- ✅ IP信息自动增强

### v1.0 (已废弃)
- ❌ 分散式日志处理
- ❌ 手动数据聚合
- ❌ Excel文件导入
- ❌ Asruex特殊处理逻辑

---

## 📞 联系与支持

如有问题,请检查:
1. 数据库配置(`config.py`)
2. 日志文件权限
3. 各服务日志输出
4. `quick_check.py`检查结果

**日志文件位置**:
- 主应用: 终端输出
- 日志处理器: `log_processor/log_processor.log`
- 数据聚合器: `stats_aggregator.log`

---

## 📋 代码整理总结

### 已移动到 `backend/old/` (废弃代码)

| 文件/目录 | 原位置 | 废弃原因 |
|----------|--------|----------|
| `asbackend/` | backend/ | 旧版Asruex后端,已被统一架构替代 |
| `ashttpd/` | backend/ | 旧版Asruex HTTPD服务,已被log_processor替代 |
| `log_db/` | backend/ | 早期日志测试代码 |
| `asruex.py` | backend/router/ | 旧版Asruex路由 |
| `data_num.py` | backend/ | 手动数据聚合脚本,已被stats_aggregator替代 |
| `generate_ip.py` | backend/ | 从Excel导入数据,已被日志上传API替代 |
| `delete_pro.py` | 根目录 | 一次性数据库清理脚本 |
| `split_botnets.py` | 根目录 | 一次性数据迁移脚本 |
| `start_all.bat` | backend/ | 旧版启动脚本,已被根目录版本替代 |
| `start_all.sh` | backend/ | 旧版启动脚本,已被根目录版本替代 |
| `stop_all.sh` | backend/ | 旧版停止脚本,已被根目录版本替代 |

### 已移动到 `backend/test/` (测试代码)

| 文件 | 原位置 | 说明 |
|------|--------|------|
| `test_processor.py` | backend/log_processor/ | 日志处理器测试 |
| `test_aggregator.py` | backend/ | 数据聚合器测试 |
| `test_db.py` | backend/ | 数据库连接测试 |
| `test_upload.py` | 根目录 | 日志上传API测试 |
| `test_api_fix.py` | 根目录 | API修复测试 |
| `remote_uploader.py` | 根目录 | 模拟C2上传客户端 |
| `quick_check.py` | 根目录 | 系统状态快速检查 |

### 当前使用的启动脚本

**推荐使用** (位于项目根目录):
- ✅ `start_all_services.bat` - Windows一键启动
- ✅ `start_all_services.sh` - Linux/Mac一键启动
- ✅ `stop_all_services.sh` - Linux/Mac一键停止

**单独启动** (位于backend目录):
- `start_aggregator.bat/sh` - 仅启动数据聚合器

---

## 🎉 总结

### 新架构的核心优势:
1. **统一处理**: 所有僵尸网络使用同一套日志处理流程
2. **自动化**: 无需手动导入数据和执行聚合
3. **实时性**: 日志实时监控和处理
4. **可扩展**: 新增僵尸网络类型只需添加日志目录
5. **高性能**: 批量写入、去重优化、异步处理
6. **易维护**: 代码模块化、文档完善、测试充分

### 代码整理成果:
- ✅ 废弃代码已归档到 `backend/old/` 目录
- ✅ 测试代码已整理到 `backend/test/` 目录
- ✅ 保留了最新、最完整的启动脚本
- ✅ 项目结构清晰,易于维护和扩展
- ✅ 修复了 `main.py` 中对废弃 `asruex` 路由的引用

### 已知的非关键性警告:
- ⚠️ Pydantic V1->V2 验证器迁移警告: 使用了旧版 `@validator` 装饰器,建议未来迁移到 `@field_validator`
  - **影响**: 仅为警告提示,不影响功能运行
  - **建议**: 长期维护时可考虑升级到 Pydantic V2 风格

**开始使用**: 在项目根目录运行 `start_all_services.bat` (Windows) 或 `./start_all_services.sh` (Linux/Mac) 即可启动完整系统!

