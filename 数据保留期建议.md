# 数据保留期建议分析报告

## 📊 服务器配置

### 硬件资源
- **CPU**: 24核
- **内存**: 62GB
- **磁盘容量**: 3.6TB（总容量）
- **已使用**: 433GB
- **可用空间**: 3.0TB (约83%可用)
- **数据库**: MySQL 8.0

---

## 📈 数据规模预估

### 业务需求
- **C2数量**: 10个
- **单C2日数据量**: 10万条/天
- **总数据量**: **100万条/天**

### 数据表结构

#### 1. `botnet_nodes_{type}` - 节点汇总表
- **用途**: 每个IP一条记录，记录节点基本信息和统计
- **字段**: IP、地理位置、ISP、ASN、通信次数、首次/最后通信时间等
- **估算大小**: 约300-400字节/记录（含索引）
- **增长特点**: 增长缓慢（仅新IP增加）

#### 2. `botnet_communications_{type}` - 通信记录表 ⭐ 主要存储
- **用途**: 记录每次通信详情（历史数据）
- **字段**: IP、通信时间、地理位置、ISP、ASN、事件类型等
- **估算大小**: 约400-500字节/记录（含索引）
- **增长特点**: 线性增长，**主要存储消耗**

#### 3. `china_botnet_{type}` 和 `global_botnet_{type}` - 统计表
- **用途**: 聚合统计数据
- **估算大小**: 可忽略不计

---

## 💾 存储容量计算

### 每日数据增长量

```
每日通信记录 = 100万条 × 500字节 = 500MB/天
每日节点汇总 ≈ 新增IP × 400字节 ≈ 50MB/天（估算）
每日总增长 ≈ 550MB/天
```

### 不同保留期的存储需求

| 保留期 | 通信记录数 | 存储空间 | 占用比例 |
|--------|-----------|---------|---------|
| **30天** | 3000万条 | ~15GB | 0.5% |
| **60天** | 6000万条 | ~30GB | 1.0% |
| **90天** | 9000万条 | ~45GB | 1.5% |
| **180天** | 1.8亿条 | ~90GB | 3.0% |
| **365天** | 3.65亿条 | ~180GB | 6.0% |
| **730天（2年）** | 7.3亿条 | ~360GB | 12% |
| **1095天（3年）** | 10.95亿条 | ~540GB | 18% |

---

## 🎯 推荐方案

### ✅ 推荐：**180天（6个月）**

#### 理由
1. **存储安全**: 约90GB，仅占可用空间的3%，留有充足余量
2. **业务价值**: 
   - 满足半年期趋势分析需求
   - 覆盖季节性变化周期
   - 支持中长期威胁情报分析
3. **性能平衡**: 
   - 1.8亿条记录在合理范围内
   - 索引性能良好（已优化）
   - 查询响应时间可接受
4. **运维成本**: 适中的数据清理频率

### 🔧 备选方案

#### 方案A：**365天（1年）** - 推荐用于长期研究
- **存储**: ~180GB（6%）
- **优点**: 完整年度数据，支持年度对比分析
- **适用**: 科研机构、长期威胁情报平台
- **风险**: 存储增长需定期监控

#### 方案B：**90天（3个月）** - 推荐用于存储敏感场景
- **存储**: ~45GB（1.5%）
- **优点**: 存储压力小，查询性能最优
- **适用**: 实时监控为主的场景
- **缺点**: 历史数据分析能力受限

---

## 📋 实施建议

### 1. 数据清理策略

#### 自动清理任务（推荐180天）
```sql
-- 清理180天前的通信记录
DELETE FROM botnet_communications_{botnet_type}
WHERE communication_time < DATE_SUB(NOW(), INTERVAL 180 DAY)
LIMIT 10000;  -- 分批删除，避免锁表

-- 建议：每天凌晨2点执行，分批清理
```

#### 定时任务配置
```bash
# 添加到crontab
0 2 * * * /home/spider/31339752/backend/scripts/cleanup_old_data.sh
```

### 2. 数据归档策略（可选）

对于重要历史数据，建议归档而非删除：

```sql
-- 归档180天前的数据到归档表
CREATE TABLE botnet_communications_{type}_archive LIKE botnet_communications_{type};

INSERT INTO botnet_communications_{type}_archive
SELECT * FROM botnet_communications_{type}
WHERE communication_time < DATE_SUB(NOW(), INTERVAL 180 DAY);

-- 然后删除原表数据
DELETE FROM botnet_communications_{type}
WHERE communication_time < DATE_SUB(NOW(), INTERVAL 180 DAY);
```

### 3. 监控指标

需要定期监控的指标：

| 指标 | 阈值 | 检查频率 |
|-----|------|---------|
| 数据库大小 | < 500GB | 每周 |
| 磁盘可用空间 | > 2TB | 每天 |
| 单表记录数 | < 2亿条 | 每周 |
| 查询响应时间 | < 3秒 | 每天 |

### 4. 性能优化建议

#### 分区表（推荐长期保留数据时使用）
```sql
-- 按月分区（适用于365天保留期）
ALTER TABLE botnet_communications_{type}
PARTITION BY RANGE (TO_DAYS(communication_time)) (
    PARTITION p202501 VALUES LESS THAN (TO_DAYS('2025-02-01')),
    PARTITION p202502 VALUES LESS THAN (TO_DAYS('2025-03-01')),
    ...
);
```

好处：
- 删除旧数据时直接删除分区，速度快
- 查询性能提升（分区裁剪）
- 维护更方便

---

## 🚀 立即行动

### 步骤1: 检查当前数据量
```bash
cd /home/spider/31339752/backend
python scripts/check_database_size.py
```

### 步骤2: 创建数据清理脚本
参考下文的清理脚本模板

### 步骤3: 测试清理流程
先在测试环境验证清理逻辑

### 步骤4: 部署自动清理
设置定时任务，启用自动清理

---

## 📝 清理脚本模板

创建文件：`/home/spider/31339752/backend/scripts/cleanup_old_data.py`

```python
#!/usr/bin/env python3
"""
自动清理旧数据脚本
保留180天数据，删除更早的通信记录
"""
import pymysql
import logging
from datetime import datetime
import sys
import os

# 添加父目录到路径
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import DB_CONFIG, BOTNET_CONFIG

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# 配置
RETENTION_DAYS = 180  # 保留天数
BATCH_SIZE = 10000    # 每批删除数量（避免锁表）

def cleanup_communications(botnet_type: str, dry_run: bool = False):
    """清理指定僵尸网络的旧通信记录"""
    table_name = f"botnet_communications_{botnet_type}"
    
    conn = None
    try:
        conn = pymysql.connect(**DB_CONFIG)
        cursor = conn.cursor()
        
        # 1. 检查要删除的记录数
        cursor.execute(f"""
            SELECT COUNT(*) FROM {table_name}
            WHERE communication_time < DATE_SUB(NOW(), INTERVAL {RETENTION_DAYS} DAY)
        """)
        total_to_delete = cursor.fetchone()[0]
        
        if total_to_delete == 0:
            logger.info(f"[{botnet_type}] 没有需要清理的数据")
            return
        
        logger.info(f"[{botnet_type}] 发现 {total_to_delete} 条需要清理的记录")
        
        if dry_run:
            logger.info(f"[{botnet_type}] 试运行模式，不执行删除")
            return
        
        # 2. 分批删除
        deleted_total = 0
        while True:
            cursor.execute(f"""
                DELETE FROM {table_name}
                WHERE communication_time < DATE_SUB(NOW(), INTERVAL {RETENTION_DAYS} DAY)
                LIMIT {BATCH_SIZE}
            """)
            deleted = cursor.rowcount
            conn.commit()
            
            if deleted == 0:
                break
            
            deleted_total += deleted
            logger.info(f"[{botnet_type}] 已删除 {deleted_total}/{total_to_delete} 条记录")
            
        logger.info(f"[{botnet_type}] 清理完成，共删除 {deleted_total} 条记录")
        
        # 3. 优化表（可选，释放空间）
        logger.info(f"[{botnet_type}] 优化表...")
        cursor.execute(f"OPTIMIZE TABLE {table_name}")
        logger.info(f"[{botnet_type}] 表优化完成")
        
    except Exception as e:
        logger.error(f"[{botnet_type}] 清理失败: {e}")
        if conn:
            conn.rollback()
    finally:
        if conn:
            conn.close()

def main():
    """主函数"""
    import argparse
    parser = argparse.ArgumentParser(description='清理僵尸网络旧数据')
    parser.add_argument('--dry-run', action='store_true', help='试运行模式（不实际删除）')
    parser.add_argument('--days', type=int, default=180, help='保留天数（默认180天）')
    args = parser.parse_args()
    
    global RETENTION_DAYS
    RETENTION_DAYS = args.days
    
    logger.info(f"========== 数据清理开始 ==========")
    logger.info(f"保留期: {RETENTION_DAYS}天")
    logger.info(f"模式: {'试运行' if args.dry_run else '正式清理'}")
    
    # 遍历所有僵尸网络类型
    for botnet_type in BOTNET_CONFIG.keys():
        if not BOTNET_CONFIG[botnet_type].get('enabled', True):
            continue
        
        logger.info(f"\n处理 {botnet_type}...")
        cleanup_communications(botnet_type, dry_run=args.dry_run)
    
    logger.info(f"\n========== 数据清理完成 ==========")

if __name__ == '__main__':
    main()
```

### 使用方法
```bash
# 试运行（不实际删除）
python backend/scripts/cleanup_old_data.py --dry-run

# 正式清理（保留180天）
python backend/scripts/cleanup_old_data.py

# 自定义保留期（如90天）
python backend/scripts/cleanup_old_data.py --days 90
```

---

## 🔍 补充说明

### 节点汇总表不建议清理
`botnet_nodes_{type}` 表记录的是节点汇总信息，不随时间线性增长，建议：
- ✅ 保留所有历史节点记录
- ✅ 定期更新节点状态（active/inactive）
- ❌ 不建议删除旧节点记录（用于历史分析）

### 统计表维护
- `china_botnet_{type}` 和 `global_botnet_{type}` 由聚合器自动维护
- 无需手动清理
- 占用空间极小

---

## 📊 总结

### 关键决策

| 项目 | 推荐值 | 说明 |
|-----|--------|-----|
| **数据保留期** | **180天** | 平衡存储、性能和业务需求 |
| **存储占用** | ~90GB | 可用空间的3%，安全范围 |
| **清理频率** | 每天凌晨2点 | 低峰期执行 |
| **清理方式** | 分批删除 | 每批10000条，避免锁表 |

### 优势
✅ 存储压力小（3%占用）  
✅ 查询性能好（1.8亿条记录）  
✅ 满足半年期分析需求  
✅ 自动化运维，无需人工干预  

### 风险控制
- 定期监控磁盘空间
- 重要数据考虑归档
- 清理前做好备份
- 测试清理脚本后再部署

---

**生成时间**: 2025-01-21  
**适用版本**: 当前平台配置（10个C2，每天100万条数据）  
**建议审核周期**: 每季度评估一次
